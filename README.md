---
abstract: |
  La storia del live electronics emerge non come una narrazione unitaria, ma come un racconto plurale, segnato da risposte profondamente diverse a un’esigenza comune: restituire alla musica elettroacustica la dimensione performativa e sociale che la tape music degli anni Cinquanta aveva inibito. Fondamentale è la distinzione tra live e real-time: se quest’ultimo è un parametro tecnico di latenza, il “dal vivo” appartiene all’ecologia dell’evento, al hic et nunc condiviso tra performer e pubblico in uno spazio fisico.
  
  Questa ricerca individua due tradizioni opposte che hanno incarnato tale esigenza. Da un lato, il modello DIY e comunitario, sviluppatosi soprattutto negli Stati Uniti attorno a figure come John Cage, David Tudor e il Sonic Arts Union, basato sull’autocostruzione, il riuso di surplus militari, la condivisione orizzontale del sapere e un’estetica dell’esplorazione e dell’imperfezione. Dall’altro, il modello istituzionale ed europeo, esemplificato dall’IRCAM di Pierre Boulez, fondato sulla centralizzazione tecnologica, sulla ricerca scientifica razionalizzata e su una complessa divisione del lavoro.
  
  Il confronto tra queste due vie rivela come le scelte tecnologiche siano sempre anche scelte organizzative, sociali e, in senso ampio, politiche. Mentre il modello istituzionale rischia di ridurre l’evento a showcase di un apparato, quello comunitario afferma la possibilità di una liveness autentica, in cui la tecnologia accessibile e modificabile serve a costruire relazioni creative dirette. Comprendere questa pluralità offre strumenti critici essenziali per interrogare il presente e immaginare futuri possibili per la musica elettroacustica.
---

# INTRODUZIONE

La storia della musica elettroacustica è spesso raccontata come una progressione lineare di innovazioni tecnologiche: dal nastro magnetico al computer, dalla sintesi analogica a quella digitale. Tuttavia, esiste una pratica che sfugge a questa narrazione puramente tecnologica e che costringe a guardare altrove: il live electronics. Se il nastro magnetico aveva confinato il compositore nello studio, relegando la performance alla semplice riproduzione di un oggetto finito, l’emergere di una prassi “dal vivo” rappresentò una rottura radicale. Ma che cosa significa, esattamente, “dal vivo” in un contesto elettroacustico? L’espressione stessa – spesso confusa con il mero “tempo reale” (real-time) – nasconde una pluralità di approcci, storie e visioni del mondo profondamente differenti.

Questa tesina muove da una domanda centrale: in quali contesti socio-culturali e geografici si sono sviluppate le prassi del live electronics, e in risposta a quali esigenze? L’ipotesi che guida la ricerca è che il live electronics non costituisca una pratica unitaria, ma si configuri piuttosto come un racconto plurale. La sua storia non è solo quella di circuiti e microfoni, ma di comunità, istituzioni, scelte estetiche e modelli organizzativi che rispondono a precise condizioni materiali e culturali. L’esigenza primaria, emersa già alla fine degli anni Cinquanta, era quella di “riportare in vita” la musica elettronica, di restituirle una dimensione performativa e sociale che la tape music aveva inibito, riconquistando quel hic et nunc – lo “qui e ora” – irriducibile dell’evento concertistico.

Per indagare questa pluralità, la tesina adotta una metodologia storico-critica e comparativa. L’analisi si basa sullo studio di testi fondativi di autori come Gordon Mumma, Joel Chadabe, Georgina Born e Agostino Di Scipio, e su ricostruzioni storiche che includono figure chiave come David Tudor, Hugh Davies e i compositori del Sonic Arts Union. Il metodo è quello del case study: attraverso l’esame di contesti specifici, si intende far emergere non una cronologia, ma una mappa di modelli opposti e complementari.

Il percorso argomentativo si articola in quattro momenti principali. Il primo capitolo affronta la necessaria distinzione concettuale tra “live” e “real-time”, chiarendo come la liveness sia una categoria legata all’ecologia performativa e alla condivisione di uno spazio, e non a un semplice parametro tecnico di latenza. Inoltre ricostruisce il contesto di crisi da cui il live electronics emerge: l’insoddisfazione per la tape music e la tensione tra il medium acusmatico e il rituale del concerto, che spinse compositori in diverse parti del mondo a cercare alternative. Infine prende in esame due risposte pionieristiche e diametralmente opposte: l’“anarchia felice” e accessibile di John Cage (Cartridge Music, 1960) e il “controllo assoluto” e specializzato di Karlheinz Stockhausen (Mikrophonie I, 1964), individuando in esse i prototipi di due filosofie destinate a divergere.

Il terzo e quarto capitolo approfondiscono questa divergenza, esplorando i due modelli maturi che hanno caratterizzato lo sviluppo successivo. Da un lato, il modello DIY e comunitario nordamericano, incarnato dal circuito come partitura, dall’etica del bricolage con surplus militare e da collettivi come il Sonic Arts Union o Composers Inside Electronics. Dall’altro, il modello istituzionale ed europeo esemplificato da IRCAM, basato sulla centralizzazione tecnologica, sulla ricerca scientifica razionalizzata e su una complessa divisione del lavoro. L’analisi mostra come entrambi i modelli rispondessero, con mezzi opposti, alla stessa esigenza di riconnettere la tecnologia musicale alla dimensione performativa.

In conclusione, questa ricostruzione mira a dimostrare che la storia del live electronics è uno specchio di visioni del mondo. Osservarne le diverse incarnazioni non è un esercizio storiografico neutro, ma un atto critico che rivela come le scelte tecnologiche siano sempre anche scelte sociali, organizzative e, in senso ampio, politiche – intese come modi di concepire la relazione tra creatività, strumenti e comunità. In un’epoca in cui la mediazione tecnologica è ubiqua, comprendere le radici plurali di questa pratica significa dotarsi di strumenti essenziali per interrogare il presente e immaginare futuri possibili per la musica elettroacustica.

# GENESI DEL LIVE ELECTRONICS

## "Live" vs "Real-time" - Una distinzione necessaria

Prima di approfondire l'analisi e dei contesti e delle prassi musicali, è essenziale chiarire una distinzione terminologica che percorrerà l'intera ricerca. In questo paragrafo, mi riferirò alle teorie della *liveness* messe in luce da pensatori come Agostino Di Scipio per sciogliere un equivoco fondamentale: i termini *live* e *real-time*, spesso usati come sinonimi, designano in realtà dimensioni profondamente differenti della pratica musicale elettroacustica.

Nella letteratura specialistica, a partire dagli anni Settanta, si è affermata una tendenza problematica, rilevata da Di Scipio, in cui le istituzioni dedicate alla computer music hanno sistematicamente preferito parlare di "computer in tempo reale" piuttosto che di "live electronics". Questa scelta terminologica privilegiava un criterio meramente tecnologico, legato alla sempre maggiore velocità computazionale, per descrivere risorse performative, introducendo un equivoco concettuale che merita chiarimento [@discipio2021thinking, 177].

La categoria del *real-time* appartiene propriamente al dominio dell'informatica e designa una capacità puramente tecnica dei sistemi: quella di elaborare dati con latenze così ridotte da risultare impercettibili. È dunque una caratteristica quantificabile, legata all'hardware e al software. Al contrario, la dimensione del "dal vivo" riguarda un ordine di fenomeni completamente diverso, legato alle condizioni irriducibili dell'evento performativo stesso. Il live electronics si definisce non attraverso parametri di latenza, ma attraverso la generazione e manipolazione contestuale del suono, mediante l'azione diretta dei performer in uno spazio condiviso con il pubblico.

Questa distinzione non è un mero tecnicismo, ma possiede implicazioni epistemologiche profonde. Come sottolinea Di Scipio, il tempo reale non è di per sé un criterio sufficiente per la *liveness*, ma deve essere integrato con una nozione di spazio reale. Ciò che definisce autenticamente l'esperienza performativa è "the indivisibility of time-space coordinates in lived experience, in fact often evoked by the common language expression the *here and now* — moulded after the Latin *hic et nunc*" [@discipio2021thinking, 177]. Una performance dal vivo si realizza all'interno di un'ecologia più ampia di azioni e percezioni, inscindibile da uno spazio fisico specifico e dalle sue connotazioni materiali e culturali.

La storia della prassi conferma questa separazione concettuale. Il live electronics, infatti, esisteva già prima dello sviluppo di computer ad alta velocità: i sistemi analogici degli anni Sessanta operavano su scale temporali diverse dagli standard odierni, ma costituivano performance autenticamente *live* perché generavano suoni nell'immediatezza dell'esecuzione. Allo stesso modo, processori digitali in grado di operare in *real-time* possono essere impiegati in produzioni in studio prive di qualsiasi dimensione performativa. Ciò che è decisivo, quindi, non è la velocità tecnica, ma la modalità d'uso: la presenza corporea, la condivisione dello spazio acustico e l'interazione con l'irripetibile contingenza del concerto.

La stessa definizione di *live electronics* incontra ulteriori problematiche. Come nota Di Scipio, la gestione di mezzi elettroacustici in concerto non nasce certo negli anni Sessanta, dato che i primi strumenti elettronici del Novecento erano concepiti per un uso dal vivo. L'espressione rappresenta piuttosto il tentativo di riunire sotto un'unica denominazione una molteplicità eterogenea di pratiche esecutive sviluppatesi in contesti geografici e istituzionali molto differenziati nel corso di decenni.

Sebbene la storiografia individui spesso in opere come *Cartridge Music* (1960) e *Mikrophonie* (1964) la nascita codificata di questa prassi, nei prossimi paragrafi ci soffermeremo sugli aspetti che ne hanno preparato il terreno, a cominciare dalla crisi della *tape music*.



## La crisi della tape music (1950-1960)

Per comprendere la portata rivoluzionaria di opere come *Cartridge Music*, è necessario ricostruire il contesto da cui emergono. Durante tutti gli anni '50, la musica elettroacustica fu quasi esclusivamente *tape music* — musica fissata su nastro magnetico e composta attraverso tecniche di editing in studio.
Come osserva Gordon Mumma:

> "During the 1950s most composers treated magnetic tape in a 'hands-on' manner analogous to that of filmmakers. As with film, tape music was composed largely through editing."[@mumma1975, p. 292]

Il nastro magnetico era "il primo supporto di registrazione per il suono che fosse ragionevolmente editabile: poteva essere tagliato e giuntato con precisione"[@mumma1975, 292].
Questa modalità di lavoro confinava i compositori in dei veri e propri laboratori con macchinari particolarmente ingombranti. Fino al 1960 vi erano pochissime eccezioni all'uso del nastro magnetico come medium esclusivamente da studio [@mumma1975, 292]. Le opere risultanti venivano poi presentate al pubblico tramite diffusione radiofonica o in concerto attraverso altoparlanti — una situazione che creava evidenti tensioni con le aspettative del pubblico da concerto.

### Il rituale del concerto

Il problema non era meramente tecnico o estetico, ma toccava il cuore stesso della presentazione musicale. I compositori che lavoravano con nastro magnetico si trovavano continuamente a sperimentare modalità per presentare le proprie opere al pubblico [@mumma1975, p. 294]. La radiodiffusione e le registrazioni domestiche funzionavano bene perché permettevano all'ascoltatore di determinare autonomamente il grado di formalità con cui ascoltare. Ma la presentazione dal vivo poneva sfide del tutto diverse: il pubblico da concerto portava con sé aspettative tradizionali consolidate: "Audiences expect to see as well as hear a performance, and loudspeakers aren't much to look at."[@mumma1975, p. 294]

Questa tensione tra il medium acusmatico — il suono privo di fonte visibile — e le convenzioni del rituale concertistico tradizionale si inseriva, come ha osservato Nicola Bernardini, in una crisi più generale del rituale del concerto stesso [@bernardini1986, 62]. L'introduzione dell'elettronica dal vivo coincise, specialmente negli Stati Uniti e in Giappone, con un momento in cui il rituale del concerto veniva rivisto, modificato, distrutto, de-strutturato praticamente per ogni nuovo brano [@bernardini1986, 62].[^1]


[^1]: Non a caso le esperienze di Cage in ambito elettroacustico, come vedremo sotto, sono inscrivibili a pratiche multi-mediali spesso con teatro, danza e luci in un panorama di sperimentazione dove la musica assolve il ruolo di un unico senso, affidando il resto alle altre arti.
vedi sezione 1.boh.

I fattori scatenanti di questa crisi erano interconnessi [@bernardini1986, 62]: l'enorme influenza di John Cage sui suoi contemporanei (paragonabile all'influenza della scuola di Darmstadt); il progressivo indebolimento del linguaggio musicale, ormai giunto ai minimi termini, che metteva in crisi la stessa legittimità del compositore; l'assenza di una prassi strumentale codificata per gli strumenti elettronici, che aprì la strada a una partecipazione del pubblico molto più attiva; l'affermarsi dell'improvvisazione libera.

In questo contesto di trasformazione radicale, numerosi compositori cercarono alternative alla presentazione tradizionale di musica su nastro, sperimentando forme di live electronics che potessero ri-animare [@discipio2021thinking] la dimensione performativa della musica elettroacustica e rispondere alle aspettative di presenza e visibilità del pubblico.

### Uscire dallo studio

Joel Chadabe apre il capitolo "Out of the Studios" del suo *Electric Sound* con una domanda e una risposta emblematiche:

> "Question: 'How do you perform electronic music without tape?' Answer (said with a shrug of the shoulders and rising inflection): 'Take away the tape.'"[@chadabe1997, p. 81]

Questa necessità di "portare la musica elettronica fuori dallo studio"[@chadabe1997, p. 81] viene osservata anche da Gordon Mumma che in un articolo passa in rassegna quelle che oggi definiremmo "best practices" per uno studio modulare, con strumentazione trasferibile dal vivo per le perfomances, è sentita anche da Stockhausen. Egli in una conferenza del 1972, definisce *Mikrophonie I* come "electronic live music" in opposizione alla "electronic music which is produced in a studio"[@roth2023, 64, citando Cott & Stockhausen 1973]. Cage, parlando di *Cartridge Music* nel 1962, aveva coniato l'espressione "live electronic music" — probabilmente per la prima volta in assoluto, come nota Di Scipio [@discipio2021, Parte Terza].
L'inversione dei termini ("live electronic" vs "electronic live") probabilmente è sintomatica di approcci di lavoro e radici culturali differenti (l'idea di musica elettronica per un compositore come Stockhausen proveniente dallo studio WDR di Colonia è di gran lunga lontana da un'idea di elettronica di Cage); ciò che accomuna entrambe le opere è tuttavia l'esigenza di *animare*[@discipio2021thinking] la musica elettronica attraverso la performance dal vivo.

### Eccezioni tardive e transizione

Verso la fine degli anni '50 iniziarono ad apparire alcune eccezioni a questo dominio esclusivo della tape music da studio [@mumma1975, 292]. Alcuni compositori sperimentarono con suoni registrati su nastro presentati in concerto dal vivo insieme a strumenti o voci, mentre altri collocarono il nastro in situazioni performative innovative che si allontanavano dai riferimenti alla musica tradizionale. Emersero anche tecniche di studio che permettevano elaborazioni instantanee configurandosi di fatto come performance dal vivo, e casi in cui il nastro veniva impiegato esclusivamente per registrare e distribuire i risultati di performance che non lo utilizzavano come medium compositivo primario [@mumma1975, 292].

Il cambio decisivo avvenne quando alcuni compositori — Cage in primis — abbandonarono il nastro come premessa musicale ed esplorarono l'uso di dispositivi elettronici, da soli o in combinazione con strumenti acustici, come componenti per la performance dal vivo [@mumma1975, 292]. Secondo Mumma, questa transizione — dal nastro come medium compositivo primario all'elettronica come strumento performativo — segnò la nascita del live electronics come prassi distinta.  

Durante gli anni '60, la performance dal vivo con suoni amplificati di piccola entità, supportata dallo sviluppo di nuova strumentazione elettronica progettata specificamente per l'uso in concerto, divenne un'attività sempre più rilevante [@mumma1975, 297]. Questa prassi emergente attrasse gradualmente l'attenzione anche di coloro che, inizialmente impegnati filosoficamente con il medium del nastro magnetico, avevano in precedenza considerato il live electronics come un'impresa di scarso interesse [@mumma1975, 297].

## Due pionieri in un panorama globale

Tra il 1960 e il 1965, la scena della musica live electronics conobbe una significativa espansione a livello internazionale. Secondo le ricostruzioni storiche, in questo periodo l'attività si concentrò principalmente negli Stati Uniti, favorita da un vivace clima sperimentale e dalla precoce accessibilità della tecnologia a stato solido [@mumma1975, p. 296]. Accanto a figure americane come Robert Ashley, Alvin Lucier, Gordon Mumma e David Tudor, fiorivano esperienze parallele in Giappone con Takahisa Kosugi, in Francia con Gil Wolman, in Italia con Giuseppe Chiari e in Germania con Karlheinz Stockhausen. Fu in questo contesto di fermento globale che due composizioni, sebbene radicalmente diverse per filosofia e metodo, divennero emblematiche della nuova direzione performativa dell'elettronica: *Cartridge Music* (1960) di John Cage e *Mikrophonie I* (1964) di Karlheinz Stockhausen.

### Un incontro a Colonia: la sfida di Cage

Nell'ottobre del 1960, la prima esecuzione ufficiale di *Cartridge Music* di John Cage ebbe luogo a Colonia, nello studio dell'artista Mary Bauermeister. Secondo le ricostruzioni più attendibili, tra il pubblico era presente Karlheinz Stockhausen [@discipio2021, Parte Terza, p. 314]. Quel concerto rappresentò un momento di potente discontinuità per il panorama elettroacustico europeo. Cage, figlio di un inventore e privo di un solido supporto istituzionale, presentava una musica elettronica costruita non con i costosi strumenti degli studi specializzati, ma con cartucce di giradischi, microfoni a contatto e oggetti quotidiani come stuzzicadenti, molle e piume [@collins2020, p. 40; @chadabe1997, p. 81-82]. L'opera rivelava un intero universo di microsuoni, reso udibile attraverso una semplice amplificazione, e la sua partitura grafica, composta da fogli trasparenti sovrapponibili, istituiva un campo di possibilità aperto all'esplorazione dei performer piuttosto che un oggetto musicale rigidamente definito [@chadabe1997, p. 82]. In questo contesto, Cage coniò per la prima volta l'espressione "live electronic music" [@discipio2021, Parte Terza, p. 314], suggellando la rottura con il paradigma della musica su nastro.

Quattro anni dopo, Stockhausen diede la sua risposta a quella stessa intuizione con *Mikrophonie I*. Se Cage aveva lanciato la sfida della "microfonia" – l'esplorazione attiva del micromondo sonoro attraverso il microfono – Stockhausen ne colse il potenziale per reindirizzarlo all'interno di una poetica del controllo assoluto. L'opera impiega sì un principio simile di amplificazione di suoni minuti, ma lo incanala in un apparato esecutivo di straordinaria complessità e precisione. Sei performer, ciascuno con un ruolo rigidamente definito, agiscono su un tam-tam di grandi dimensioni: due lo eccitano con una gamma di oggetti, due ne esplorano la superficie con microfoni direzionali seguendo scrupolose indicazioni spaziali, e due manipolano filtri e potenziometri per trasformare il suono in tempo reale [@chadabe1997, p. 83-84; @mooney2017]. L'equipaggiamento non è più quello della tecnologia quotidiana, ma è altamente specializzato, incluso un raro filtro Maihak e un tam-tam Paiste appositamente selezionato [@roth2023, p. 61-66]. La partitura, basata sul principio della "moment-form", prescrive nel dettaglio ogni parametro, dalle distanze microfoniche alle frequenze dei filtri, trasformando l'intuizione esplorativa di Cage in un architettura sonora deterministicamente controllata [@chadabe1997, p. 83-84; @roth2023, p. 64].

### Una ricerca comune, due visioni del mondo

Entrambe le opere condividono la scoperta del microfono come strumento attivo, capace di rivelare un "nuovo mondo di risorse sonore" [@mumma1975, p. 296]. La ricchezza dei microsuoni amplificati in *Cartridge Music* era tale da rivaleggiare, come notato da Nicolas Collins, con le sonorità più laboriose prodotte negli studi elettronici europei [@collins2020, p. 40]. Tuttavia, le filosofie sottostanti non avrebbero potuto essere più distanti. Come sintetizza efficacemente Joel Chadabe, la musica di Cage proponeva un'"anarchia felice" basata su processi e giustapposizioni casuali, mentre quella di Stockhausen comunicava "controllo, tecnica ed expertise" [@chadabe1997, p. 83]. Questa divergenza è palpabile nelle loro dichiarazioni: Cage affermava che la sua partitura poteva servire a "esaminare musicalmente una vecchia Volkswagen" [@roth2023, p. 64], mentre Stockhausen sosteneva con forza che sostituire i filtri originali con simulazioni avrebbe distrutto l'unicità storica dell'opera [@roth2023, p. 64].

Questa divergenza di approccio si riflette persino nella terminologia scelta da ciascun compositore per definire la propria musica. Cage, già nel 1962, aveva coniato l'espressione 'live electronic music', ponendo l'accento sull'esperienza performativa ('live') come qualificante principale della musica ('electronic music'). Stockhausen, presentando Mikrophonie I nel 1972, la definì invece 'electronic live music', descrivendo così un genere musicale elettronico ('electronic music') che semplicemente avviene in diretta ('live'). Questa sottile ma significativa inversione lessicale cristallizza le due visioni: da un lato la performance come condizione esistenziale e filosofica, dall'altro il 'dal vivo' come modalità esecutiva di un'opera strutturata.

Sebbene non si possa stabilire un nesso causale certo, il potenziale incontro del 1960 a Colonia cristallizza un passaggio cruciale nella storia del live electronics. *Cartridge Music* funse da potente agente destabilizzante, introducendo nel cuore dell'Europa colta un'idea di elettronica povera, processuale e filosofica. *Mikrophonie I* rappresentò la risposta di un compositore istituzionale, che assorbì il principio della "microfonia" ma lo tradusse nel linguaggio del rigore, della complessità strutturale e del controllo tecnologico specialistico. Come osserva Agostino Di Scipio, erano entrambi "tempi di microfonia, anche se di tipo molto diverso" [@discipio2021, Parte Terza, p. 314]. Da questa comune intuizione nacquero così due traiettorie fondative, destinate a influenzare in modo duraturo lo sviluppo della musica elettroacustica dal vivo.


## Oltre la diade: accenni a un panorama plurale

Sebbene *Cartridge Music* e *Mikrophonie I* dominino la storiografia del live electronics, incarnando rispettivamente l'approccio DIY americano e il modello istituzionale europeo, la loro non fu una rivoluzione solitaria. Tra il 1960 e il 1970, ricerche parallele fiorivano in contesti geografici e culturali profondamente diversi, ciascuna elaborando soluzioni originali al problema di liberare la musica elettronica dal nastro magnetico e riportarla alla dimensione performativa del concerto.

In Italia, Luigi Nono trovò all'ExperimentalStudio der Heinrich-Strobel-Stiftung des Südwestfunks di Friburgo un modello alternativo sia rispetto agli studi radiofonici italiani che alle grandi istituzioni come IRCAM. Dal 1980, come ospite permanente dello studio — e dal 1983 come "artistic advisor" — Nono sviluppò un approccio basato sulla residenza artistica prolungata, trascorrendo ore innumerevoli in studio insieme a interpreti e tecnici in un processo di sperimentazione continua. Come avrebbe ricordato Hans Peter Haller, le parole ricorrenti di Nono erano "unsteady – emotional – searching – always different – always better". Questo "ecosistema di Friburgo" produsse, in meno di un decennio, una serie impressionante di capolavori che spaziavano dal colossale *Prometeo – Tragedia dell'Ascolto* alle più intime *Risonanze erranti*, opere in cui tecniche come i delay multipli e asimmetrici, i pitch shift microtonali e la dislocazione del suono nello spazio divennero la "firma" del compositore. Il modello Nono rappresentava una terza via: né il grande centro di ricerca centralizzato né l'autocostruzione DIY, ma piuttosto un'infrastruttura esistente disposta a investire tempo e risorse per permettere al compositore di sviluppare lentamente una forte visione estetica attraverso la sperimentazione profonda.

Negli Stati Uniti, intorno a figure come Gordon Mumma e Robert Ashley nella città di Ann Arbor, si stava formando un intero ecosistema basato sull'autocostruzione, la collaborazione orizzontale e la portabilità delle tecnologie. Nel 1958, Ashley e Mumma fondarono il Cooperative Studio for Electronic Music, uno studio domestico low-cost esplicitamente concepito per essere accessibile senza affiliazioni istituzionali o ingenti risorse economiche. Il Cooperative Studio forniva musica per filmmaker indipendenti, progettava "cybersonic equipment" per performance dal vivo, e incarnava un principio fondamentale: sacrificando la calibrazione precisa delle grandi attrezzature di laboratorio a favore delle dimensioni compatte dei componenti hi-fi ad alta impedenza, era possibile configurare un sistema dove "l'intero studio fosse alla portata del compositore comodamente seduto" [@mumma2015, p. 19-20].

Questo spirito cooperativo trovò espressione istituzionale nell'ONCE Festival (1961-1965), che presentò complessivamente ventinove concerti con sessantasette prime esecuzioni di circa centocinquanta opere di ottantotto compositori contemporanei [@mumma2015, p. 30]. Significativamente, quando alcuni membri del gruppo tentarono di ottenere supporto dalla University of Michigan, incontrarono "resistenza e persino animosità", con un boicottaggio quasi unanime dei concerti da parte dei docenti della School of Music durante il festival del 1963 [@mumma2015, p. 24, 29-30]. Il supporto venne invece dal Dramatic Arts Center, un'organizzazione locale non universitaria che manteneva un rapporto diretto e immediato con la comunità, senza overhead amministrativo [@mumma2015, p. 30]. L'ONCE Festival non era isolato, ma parte di un ricco ecosistema culturale che includeva lo Space Theatre di Milton Cohen (le cui produzioni culminarono alla Biennale di Venezia nel 1964), l'Ann Arbor Film Festival, e il Sonic Arts Union — quest'ultimo formato nel 1966 quando David Behrman e Alvin Lucier incontrarono Ashley e Mumma durante l'ONCE Festival del 1964 [@chadabe1997, p. 102].

Anche l'Italia vide fiorire scene parallele. A Roma, nel 1964, si formò il Gruppo di Improvvisazione Nuova Consonanza, una collaborazione internazionale che includeva i compositori americani Larry Austin, John Eaton e William Smith insieme agli europei Mario Bertoncini, Aldo Clementi, Franco Evangelisti, Roland Kayn e Ivan Vandor [@mumma1975, p. 316]. Nel 1966, sempre a Roma, venne organizzato MEV (Musica Elettronica Viva), ensemble principalmente di musicisti americani — tra cui Frederic Rzewski, Alvin Curran e Richard Teitelbaum — che nel giro di due anni passò dalla musica composta all'improvvisazione libera fino ad abbandonare nel 1968 "interamente le strutture musicali e sociali formali" [@mumma1975, p. 316]. A Firenze, Giuseppe Chiari compose dal 1964 al 1966 una serie di "azioni musicali" che facevano "largo impiego di microfoni a contatto" [@bernardini1986, p. 68].

Il Giappone sviluppò una scena particolarmente significativa. Takahisa Kosugi compose *Micro 1* (1961), opera per solo microfono, seguita da una serie di lavori poetici con elettronica radio-frequency e audio-frequency [@mumma1975, p. 299]. Toshi Ichiyanagi, che aveva studiato a New York con John Cage tra il 1956 e il 1961, al suo ritorno in Giappone compose un repertorio di opere per strumenti occidentali e giapponesi modificati elettronicamente, tra cui *Space* (1966), *Activities for Orchestra* (1967) e *Music for Living Spaces* (1970), quest'ultima un'installazione per l'Expo di Osaka [@mumma1975, p. 299]. Allo Studio NHK di Tokyo, Joji Yuasa utilizzò uno speciale magnetofono a cinque piste per opere come *Icon (on the source of white noise)* (1967), con diffusione del suono variabile da cinque a venticinque canali indipendenti [@discipio2021, Parte Seconda, p. 273]. La convergenza tra sperimentazione americana e giapponese trovò espressione simbolica nel Cross Talk Intermedia Festival, presentato a Tokyo nel febbraio 1969 nella struttura olimpica di Kenzo Tange progettata da Kenzo Tange, che vide la partecipazione di Mumma, Ashley, Lucier, Reynolds e VanDerBeek insieme a Ichiyanagi, Kosugi, Takemitsu e Yuasa [@mumma1975, p. 318].

Nel Regno Unito, la scena del live electronics si sviluppò con caratteristiche distintive rispetto alle controparti americana ed europea continentale. Il gruppo AMM, formatosi a Londra nel 1965 con Lou Gare, Keith Rowe ed Edwin Prévost (poi raggiunto da Cornelius Cardew nel 1966), incarnò un approccio radicalmente improvvisativo che, pur condividendo con MEV l'uso di elettronica dal vivo, poneva l'accento su preoccupazioni "più sociali che tecnologiche" [@appleton1975, p. 317]. La prassi di AMM consisteva principalmente nell'uso di microfoni a contatto su oggetti quotidiani e strumenti elettromeccanici autocostruiti realizzati riciclando materiale elettronico di scarto [@prevost1995]. Nonostante il retroterra jazzistico dei fondatori, il gruppo cercò di evitare stilemi idiomatici specifici, puntando sull'improvvisazione come strategia per esplorare sonorità trascurate e marginali [@discipio2021, p. 329] – un approccio che Cardew avrebbe teorizzato nel suo influente saggio *Towards an Ethic of Improvisation*, pubblicato all'interno del *Treatise Handbook* [@cardew1971].

Parallelamente, Hugh Davies, dopo aver lavorato come assistente di Stockhausen a Colonia (1964-66), sviluppò a Londra una prassi di costruzione di strumenti elettroacustici da materiali di recupero [@mooney2017]. I suoi *Shozyg* (1968) e *Springboard* (1970-74) – assemblati da molle, lame da seghetto, affettauova – incarnavano quella che Davies definì una filosofia dei "sintetizzatori di musique concrète": strumenti capaci di generare istantaneamente sonorità che negli studi su nastro avrebbero richiesto ore di lavoro [@mooney2017]. Davies propugnava "un *bricolage* elettroacustico molto competente, secondo un approccio di auto-costruzione e auto-gestione dei mezzi che contrastava nettamente con la standardizzazione dei dispositivi di produzione industriale" [@discipio2021, p. 329]. La sua partecipazione a gruppi come *Gentle Fire* e *Music Improvisation Company* contribuì a formare un ecosistema britannico caratterizzato da un DIY working-class e da una forte dimensione improvvisativa, distinguendosi così dalle esperienze sperimentaliste sia del modello istituzionale europeo continentale che dalle formazioni cooperative americane [@born1995, pp. 59-60].

Questo ricco e policentrico contesto rivela come il live electronics non fosse semplicemente l'antitesi della tape music, ma rappresentasse piuttosto un campo di sperimentazione multiplo in cui convergevano preoccupazioni estetiche, necessità tecnologiche, modelli organizzativi e visioni culturali profondamente diverse. I compositori attivi in questi vari contesti condividevano l'urgenza di superare la fissazione su nastro e recuperare la dimensione performativa, ma elaboravano soluzioni radicalmente diverse: dalla residenza artistica prolungata di Nono a Friburgo, agli studi cooperativi e ai festival indipendenti di Ann Arbor, dai collettivi improvvisativi romani alle collaborazioni transpacifiche. È a questo ecosistema nordamericano — caratterizzato dall'autocostruzione dei circuiti, dalla collaborazione orizzontale, dalla portabilità delle tecnologie e dalla necessità trasformata in filosofia — che dedicheremo il prossimo capitolo, esplorando come figure come Gordon Mumma, David Tudor, Alvin Lucier e i membri del Sonic Arts Union abbiano elaborato non solo nuove tecnologie, ma un modello completamente alternativo di prassi compositiva e performativa.

# Circuiti come partiture: tecnologia DIY e poetiche compositive (1960-1976)
## Contesto tecnologico-economico: dalla necessità alla scelta estetica

### Il transistor come rivoluzione tecnica

La nascita del live electronics americano negli anni Sessanta fu resa possibile da una rivoluzione tecnologica fondamentale: l'invenzione del transistor nel 1947 ai Bell Laboratories e la sua successiva commercializzazione. Come spiega Agostino Di Scipio, fu solo a metà del secolo che divennero disponibili materiali semiconduttori adatti a creare componenti circuitali compatti e versatili. L'elettronica precedente, basata sulle valvole termoioniche, non era riuscita a produrre sistemi musicali sufficientemente stabili ed efficienti, utili in contesti generici e non legati all'esperienza del singolo inventore. La nuova elettronica a transistor, sfruttando il rapporto esponenziale tra tensione in ingresso e corrente in uscita, si presentava invece come la soluzione a quei problemi, aprendo la porta a "possibilità applicative prima impensabili"[@discipio2021, 279].

Questa transizione tecnologica ebbe conseguenze dirette per i compositori. David Behrman ricorda che verso il 1965, lavorare con circuiti autocostruiti era una necessità, poiché non c'erano *synth* disponibili in vendita nel mercato. Tuttavia, Behrman aggiunge una nota cruciale: "You didn't have to have an engineering degree to build transistorized music circuits"[@collins2020, Foreword]. I transistor non solo miniaturizzarono l'elettronica, ma la democratizzarono, rendendola accessibile.

### Il surplus bellico come materiale compositivo

La Seconda Guerra Mondiale lasciò in eredità un'abbondanza di componenti elettronici militari dismessi, disponibili a prezzi irrisori. Gordon Mumma ricorda come David Tudor passasse molto tempo nei negozi di surplus militare a procurarsi transistor e condensatori insoliti. Assemblandoli, si otteneva un circuito unico—un oscillatore, un modulatore—la cui risposta sonora non era replicabile. Sostituendo anche un solo transistor con un altro, il circuito cambiava, dotando ogni strumento di una personalità sonora specifica e irripetibile[@nakai2021, 173-174].

Questa caratteristica—l'*irriproducibilità* intrinseca dei componenti surplus—non fu vista come un limite, ma divenne un valore estetico. You Nakai sintetizza: "Specific components composed specific instruments, which in turn composed specific sound systems—so it was *bias all the way down*"[@nakai2021, 174]. L'approccio americano si distingueva radicalmente dagli studi europei istituzionali: dove Colonia o l'IRCAM perseguivano la precisione e la riproducibilità attraverso equipaggiamento professionale costoso, la scena americana abbracciava l'imperfezione produttiva dei componenti di scarto.

Il contesto economico era tale che, come ricorda Robert Ashley, non si aveva accesso a nulla, se non all'elettronica di base. Prima della fine degli anni Sessanta, i sintetizzatori commerciali semplicemente non esistevano e il mercato era dominato da rivenditori come Lafayette Radio Electronics, che vendeva kit per hobbisti a prezzi accessibili. È significativo che David Tudor conservasse meticolosamente le ricevute per l'acquisto di kit come l'amplificatore a tre transistor PK-522 e il mixer microfonico PA-292, acquistati per pochi dollari[@nakai2021, 121].

Quando i primi sintetizzatori commerciali, come il Moog e il Buchla, arrivarono sul mercato, molti compositori del Sonic Arts Union li rifiutarono esplicitamente. Gordon Mumma tracciava una distinzione fondamentale tra la "explorer tendency" dei musicisti della Cunningham Dance Company e l'omogeneità rappresentata dalla produzione di massa e dai synth commerciali. In questa visione, "The idea of 'product' was fundamental in that regressive cultural tide"[@chadabe1997, 102].

### John Cage: Dal Prepared Piano ai Microsuoni Amplificati

Se nel capitolo precedente John Cage è emerso come pioniere a livello globale, in dialogo e contrapposizione con Stockhausen, per comprendere la specificità del modello americano è necessario considerarlo come l'ispiratore filosofico di un intero movimento (infatti come vedremo in seguito è poi la figura di David Tudor a catalizzare le proposte di Cage in una nuova prassi compositiva). La sua ricerca, iniziata ben prima del seminale *Cartridge Music* (1960), gettò le basi per un approccio alla tecnologia musicale che fosse accessibile, processuale e radicalmente performativo.

La formazione di Cage fu atipica e profondamente segnata da una familiarità con la tecnologia fin dagli anni '30. Figlio di un inventore, assisteva il padre nelle ricerche per i brevetti, sviluppando "una sufficiente padronanza dei concetti e del vocabolario del campo per comunicare con i professionisti" [@mumma2015, 167]. Questo retroterra spiega la sua disinvoltura nell'approccio alla tecnica. La sua curiosità per i nuovi media si manifestò precocemente: tra il 1932 e il 1933 presentava programmi radiofonici a Los Angeles, e nel 1936 lavorava come apprendista montatore con il filmmaker Oskar Fischinger, sperimentando tecniche di sincronizzazione suono-immagine che anticipavano il lavoro sul nastro magnetico [@mumma2015, 167].

Il percorso di Cage verso il live electronics non fu un fulmine a ciel sereno, ma il culmine di una ricerca ventennale sul suono, la performance e la tecnologia, caratterizzata da una grande coerenza di pensiero.

1.  Percussioni e Prepared Piano (anni '40): Già nel 1939, con *First Construction in Metal*, Cage utilizzava "ceppi dei freni e altro ferro vecchio recuperato dalle discariche" (Collins, 2020, p. 40). Il ricorso al prepared piano nello stesso decennio, oltre a creare sonorità inedite, rispondeva a un'esigenza pratica: "avere una risorsa multi-timbrica senza il duro lavoro di spostare strumenti a percussione" (Mumma, 2015, p. 162). L'idea di trasformare oggetti di uso comune in strumenti musicali era già pienamente presente.

2.  La serie "Imaginary Landscape" (1939-1952): Questa serie rappresenta il vero e proprio laboratorio delle sue future idee. *Imaginary Landscape No. 1* (1939) è storicamente significativa come "il primo brano documentato di musica a presentare il DJ come un performer musicale" (Collins, 2020, p. 40), utilizzando giradischi a velocità variabile. Di Scipio (2021a) nota come l'uso performativo del grammofono fosse un'idea dadaista che Cage mutuò da László Moholy-Nagy. Un passaggio cruciale avvenne nel 1942 con *Imaginary Landscape No. 3*, che segnò "il primo uso di piccoli suoni amplificati" da parte di Cage [@mumma2015, 169]. La serie culmina con *Imaginary Landscape No. 4* (1951) per dodici radio, che, insieme a *Radio Music* (1952) e *Music Walk* (1958), esplorò sistematicamente "il ricevitore radio come strumento per la performance dal vivo" (Mumma, 1975, p. 293).

3.  Il periodo del nastro: *Williams Mix* (1952): Sebbene si tratti di un'opera su nastro, *Williams Mix* è fondamentale per la sua metodologia. Cage, con l'aiuto di Earle Brown e David Tudor, creò una biblioteca di suoni catalogati (città, campagna, suoni elettronici, etc.) e utilizzò l'I Ching per decidere tramite operazioni casuali "che tipo di suono utilizzare, su quali tracce posizionarlo, e le durate di suoni e silenzi" (Chadabe, 1997, p. 56). Questo approccio, volto a definire un "territorio" sonoro (in questo caso, il mondo) piuttosto che un oggetto musicale finito, sarebbe diventato centrale nella sua filosofia (Chadabe, 1997, p. 83).

Tutti questi fili si riuniscono in *Cartridge Music*. Alla fine degli anni '50, Cage coinvolse David Tudor nella ricerca di trasduttori elettronici, componenti che andavano diffondendosi con la tecnologia a transistor [@mumma2015, 170]. 
Come nota Gordon Mumma, questo lavoro, assieme alla Music for Amplified Toy Pianos, fu eseguito frequentemente da Cage e David Tudor, essendo "a considerable stimulus to experimentation in live-electronic music"[@mumma1975, 295]. Nicolas Collins sintetizza che la sorprendente ricchezza di questi "microsuoni" enormemente amplificati rivaleggiava con le sonorità sintetiche e costose degli studi europei e "opened the ears of a generation of sound artists to the splendor of the contact mike"[@collins2020, 40].
La partitura di Cartridge Music si basava su un sistema indeterminato ma strutturato. Essa consisteva in materiali grafici su fogli trasparenti che i performer potevano sovrapporre in combinazioni diverse per definire la struttura di una specifica esecuzione. Questo approccio permetteva a Cage di strutturare diverse situazioni performative, incluse quelle collaborative.
David Tudor affrontò queste partiture grafiche con un rigore quasi scientifico, sviluppando un sistema personale per la loro interpretazione. Come documenta You Nakai, Tudor creò template e righelli su misura e elaborò "nomographs" (tradotto nomografi)—sistemi di misurazione grafica—applicando una polarità semplice/complesso a sei parametri musicali[@nakai2021, varie pagine]. In questo modo, la notazione grafica non eliminava la precisione, ma la spostava dalla prescrizione del compositore all'interpretazione attiva e metodica del performer.

L'eredità di Cage per il modello americano che di lì a poco sarebbe fiorito ad Ann Arbor è quindi triplice: filosofica (L'apertura a tutti i suoni e l'uso di processi indeterminati), pragmatica (L'uso della tecnologia quotidiana e accessibile, in opposizione agli apparati degli studi istituzionali) e sociale (La definizione di un "territorio" performativo che valorizza l'esplorazione collettiva).
Queste idee, concretizzatesi nella prassi di David Tudor, avrebbero trovato nel fertile ecosistema di Ann Arbor il terreno ideale per evolversi in un vero e proprio movimento.

## Circuit Music: lo schema circuitale come partitura

La *circuit music* costituisce un paradigma compositivo fondamentale per il live electronics nordamericano. Come definito da Ezra Teboul, essa è caratterizzata dall'adozione di schemi circuitali e diagrammi come sistema notazionale primario, in luogo della partitura tradizionale [@Teboul2023]. In questa prospettiva, il circuito cessa di essere un mero mezzo esecutivo per divenire esso stesso la partitura, ridefinendo la figura del compositore come progettista di sistemi generativi di comportamenti sonori. Nei paragrafi seguenti, questo principio verrà esaminato attraverso le pratiche di coloro che ne sono stati gli interpreti più significativi.

### David Tudor: il caso paradigmatico

David Tudor incarna in modo esemplare l'etica della circuit music. La sua filosofia si basava su un principio di scoperta e ascolto, sintetizzato nella celebre affermazione: *"I try to find out what's there—not to make it do what I want, but to release what's there. The object should teach you what it wants to hear"* [@collins2004]. Questo ribaltamento della relazione tra compositore e strumento, in cui il materiale guida il processo creativo, divenne cardine della pratica performativa. Il suo metodo empirico valorizzava la scoperta diretta delle proprietà intrinseche dei componenti, al di là delle loro specifiche tecniche [@nakai2021, 55].

Questa filosofia maturò attraverso una radicale transizione: da acclamato interprete pianistico delle avanguardie a pioniere autonomo del live electronics. Il punto di svolta fu la sua realizzazione di *Variations II* di Cage, dove il pianoforte amplificato fu concepito non come strumento acustico con elettronica aggiunta, ma come un *autentico strumento elettronico*, il cui comportamento sonoro, plasmato da microfoni a contatto e circuiti di feedback, diveniva intrinsecamente indeterminato.

L'affermazione di questa nuova identità compositiva si dichiarò formalmente in *Fluorescent Sound* (1964), che You Nakai identifica come *"the first work in which he credited himself as a 'composer'"* [@nakai2021, 210-211]. L'opera, manipolando circuiti di luci fluorescenti, esemplifica la logica della circuit music anche nel suo titolo, dove una categoria di componente diviene, per sineddoche, il nome di un'opera. La piena consacrazione giunse con *Bandoneon!* (1966). Nonostante l'esordio fosse un disastro tecnico a causa di circuiti cablati al contrario, l'episodio confermò Tudor come "the ultimate performer", capace di trasformare la precarietà del sistema in un elemento costitutivo della performance.

### Altri volti della circuit music: Mumma, Lucier, Ashley, Behrman

Oltre a Tudor, altri compositori declinarono il principio della circuit music in direzioni distintive, arricchendo il panorama del live electronics nordamericano.

Gordon Mumma sviluppò l'approccio *cybersonico*, un sistema autoreattivo in cui il processamento elettronico modificava il suono attraverso caratteristiche derivate dal suono stesso, creando complessi loop di feedback. Opere come *Hornpipe* (1967) realizzavano questa idea in forma portatile, con un circuito che rispondeva dinamicamente alle caratteristiche del corno. La sua Cybersonics Company (1965) incarnò lo spirito collaborativo e artigianale della scena, producendo dispositivi personalizzati come lo *Spectrum Transfer* per David Tudor. La collaborazione tra i due culminò in *Mesa* (1966), per bandoneon ed elettronica, presentata al seminale evento *9 Evenings: Theatre & Engineering*, simbolo dell'integrazione tra arte, scienza e hardware.

Alvin Lucier orientò la sua ricerca verso l'esplorazione di fenomeni fisici puri. In *I Am Sitting in a Room* (1969), utilizzò il feedback acustico come materiale compositivo, trasformando progressivamente la voce parlata nelle risonanze dello spazio. In *Music for Solo Performer* (1965), le onde cerebrali amplificate (EEG) attivavano strumenti a percussione, inserendo il performer in un sistema di feedback biologico ed elettroacustico. In entrambi i casi, Lucier agiva come progettista di un sistema in cui agenti non-umani—la risonanza ambientale o l'attività cerebrale—diventavano i veri interpreti [@mumma2015; @collins2004].

Robert Ashley sviluppò invece un *electronic music theater*, spostando l'attenzione dalla produzione musicale in senso stretto alla creazione di "situazioni sonore" teatrali. Opere come *The Wolfman* (1964) e *in memoriam... Crazy Horse* (1964) usavano il feedback vocale estremo come drammaturgia, dove il performer diventava un "operatore di feedback" che interagiva deliberatamente con l'instabilità del sistema elettronico [@chadabe1997, 87].

Infine, David Behrman esplorò sistemi a *indeterminacy incorporata*, dove l'imprevedibilità era iscritta nell'architettura del circuito. In *Runthrough*, l'uso di fotocellule permetteva di generare suoni attraverso le ombre dei performer, creando una composizione la cui forma precisa emergeva da azioni imprevedibili [@collins2020]. Behrman inserì esplicitamente questa pratica nella tradizione americana della costruzione strumentale, tracciando una linea ideale da Harry Partch a Tudor e Mumma, e collocando così l'hacking hardware in una precisa genealogia culturale [@collins2020, Foreword].

Questi approcci diversi—dall'autoreattività cybersonica all'ascolto di fenomeni fisici, dal teatro del feedback ai circuiti semi-autonomi—dimostrano come il paradigma della circuit music abbia offerto un comune terreno operativo per esplorazioni radicalmente differenti, pur nella condivisione di un principio fondamentale: la progettazione di sistemi, non la scrittura di suoni.

### L'organizzazione sociale come risorsa

La risposta alla mancanza di supporto istituzionale e di risorse economiche fu l'invenzione di modelli organizzativi basati sulla cooperazione. Il primo e fondamentale esperimento fu il Cooperative Studio for Electronic Music, fondato da Gordon Mumma ad Ann Arbor nel 1960. Questo studio, il primo indipendente del suo genere negli Stati Uniti, si basava su un'idea esplicitamente politica di condivisione delle risorse. La sua natura si distingueva radicalmente dai modelli europei per la mobilità dell'attrezzatura, resa possibile dai transistor, che permetteva di trasformare qualsiasi spazio in un luogo di performance e ricerca sonora [@mumma2015].

Questo spirito collettivo trovò presto una sua vetrina pubblica e internazionale nel ONCE Festival (1961-1965), che operava deliberatamente ai margini delle istituzioni, utilizzando spazi non convenzionali come magazzini e scantinati in una chiara presa di posizione contro il sistema musicale accademico. Da questa matrice comunitaria emerse, nel 1966, una formazione più strutturata: il Sonic Arts Union, fondato da Mumma, Robert Ashley, David Behrman e Alvin Lucier. Il gruppo univa compositori dalle estetiche molto diverse, accomunati dall'interesse per tutti i fenomeni sonori e da una prassi profondamente collaborativa, in cui eseguivano e adattavano le opere gli uni degli altri [@chadabe1997, 102]. I loro estenuanti tour europei furono fondamentali per esportare l'approccio americano, prima che il collettivo si sciogliesse nel 1976, quando i suoi membri furono assorbiti dal mondo accademico.

La filosofia della condivisione della conoscenza e del *learning-by-doing* trovò la sua più piena espressione nella generazione successiva, con il collettivo Composers Inside Electronics, formatosi nel 1973 attorno a un workshop sull'opera *Rainforest* di David Tudor. Come spiega Nicolas Collins, il metodo era empirico e orizzontale: si imparava smontando e replicando gli strumenti del maestro [@collins2004]. Questo modello pedagogico-collaborativo codificò l'etica dell'hacking hardware come un sapere comune, trasmesso attraverso workshop e reti informali. La conoscenza circolava orizzontalmente, con schemi circuitali condivisi "like samizdat literature", formando un patrimonio comune al di fuori della logica della proprietà intellettuale [@collins2020]. Era, come preferisce definirla Collins, una pratica di *DIT (Do It Together)* più che di semplice DIY, che trasformava la precarietà delle condizioni in una rete duratura di collaborazioni.

Questa infrastruttura sociale cooperativa alimentava una precisa posizione estetica e ideologica di resistenza. Il rifiuto dei sintetizzatori commerciali come il Moog e il Buchla non era solo pratico, ma filosofico. David Behrman vi individuava il rischio di un’estetica del cliché, mentre Gordon Mumma vi leggeva una regressione culturale verso la logica del "prodotto" standardizzato [@chadabe1997, 102; @collins2020].
Questa presa di posizione si formalizzò in un’estetica "post-ottimale", teorizzata da Ezra Teboul come scelta deliberata dell'inefficienza e del riuso, uno sguardo su una tecnologia liberata dalla ragione del profitto [@Teboul2023]. La pratica di impiegare componenti surplus e circuiti autocostruiti si configurava così come un gesto critico, allineandosi a una più ampia *estetica del junk* che, nell’arte americana, trasformava lo scarto in commento sociale. In quest’ottica, la precarietà performativa—esemplificata dal fallimento tecnico di *Bandoneon!*—diveniva il tratto distintivo di un approccio che valorizzava l’esplorazione e l’instabilità sopra l’affidabilità omogenea del commerciale.

### Dalla partitura al network: una ridefinizione dell’atto compositivo

La convergenza di questi principi—circuit music, cooperazione, post-optimalità—ridefinì l’atto compositivo nelle sue fondamenta. Come rifletteva David Tudor, si passò dal comporre musica al *comporre strumenti* che generano musica. Questo spostamento inaugurò una forma di *co-composizione*, sfumando i confini d’autorialità. La celebre massima di Tudor, *"the object should teach you what it wants to hear"* [@collins2004], attribuiva un’agency decisiva al dispositivo: il circuito, con le sue specifiche proprietà e *bias*, diventava un collaboratore attivo.

In definitiva, fu l’organizzazione sociale stessa a emergere come forma compositiva allargata. La rete di conoscenze, strumenti e pratiche condivise costituiva un vero e proprio *commons* tecnologico, un ecosistema alternativo descritto da Nicolas Collins come "part of a community of shared knowledge" anziché proprietà intellettuale [@collins2020, 6]. Questo modello, anticipando di decenni le logiche del *software libero* e della produzione *peer-to-peer*, sanciva il passaggio dalla musica della notazione alla *musica del network*. Il circuito fungeva così simultaneamente da partitura e da protocollo per una cultura collaborativa, fondando l’eredità radicale di questa scena.

## Il modello britannico: improvvisazione e materialità

Come accennato nel capitolo precedente, la scena londinese di fine anni Sessanta sviluppò un approccio al live electronics che,pur condividendo con la tradizione americana la necessità economica dell'autocostruzione e il rifiuto della mediazione istituzionale, articolò queste istanze con specifiche modalità organizzative e priorità estetiche [@discipio2021, p. 329; @born1995, pp. 59-60]. Se i compositori del Sonic Arts Union avevano elaborato un modello di cooperativa performativa centrata sulla tecnologia come linguaggio compositivo, figure come Hugh Davies e gruppi come AMM enfatizzarono invece la dimensione radicalmente improvvisativa e la materialità degli oggetti quotidiani come agenti sonori. Entrambi i contesti condivisero la pratica del bricolage elettronico e l'impiego di componenti di scarto, ma mentre gli americani attinsero principalmente al surplus militare, la scena britannica privilegiò materiali domestici, sviluppando una sensibilità working-class che negli anni Settanta si sarebbe esplicitata in termini ecologici e politici. Vale ora approfondire queste convergenze e specificità, illuminando come le condizioni economiche e istituzionali simili producessero soluzioni tecniche affini ma filosofie performative distintive.

### Hugh Davies e la filosofia del materiale di scarto

Dopo due anni come assistente di Karlheinz Stockhausen a Colonia, Hugh Davies si trovò, al suo rientro a Londra nel 1967, in una situazione paradossalmente simile a quella dei colleghi americani. Privato dell'accesso all'attrezzatura sofisticata dello studio tedesco e senza le risorse per procurarsene una propria, Davies trasformò questo limite in un catalizzatore creativo.

La svolta avvenne grazie all'incontro con Annea Lockwood, che gli mostrò il potenziale musicale di oggetti comuni, come dei fischietti per bambole, aprendolo a un universo sonoro "far removed from the avantgarde context I had largely worked in beforehand" [@davies2002soundsheard, 55]. Da qui, tra il 1968 e il 1974, sviluppò la sua pratica costruttiva, dando vita agli strumenti della serie *Shozyg* e *Springboard*. Questi assemblaggi amplificati utilizzavano oggetti di recupero domestico—dalle molle metalliche alle scatole del pane—attivati da microfoni a contatto e pickup ricavati da vecchie cornette telefoniche.

Il documentarista David Roberts notò come Davies adottasse sistematicamente un linguaggio che attribuiva agency allo strumento, usando frasi come "the instrument tells me what to do". Questa filosofia, che vedeva nel materiale un interlocutore attivo, lo avvicinava all'approccio dei compositori americani, pur applicandola a un repertorio di oggetti radicalmente diverso e quotidiano.

Da necessità pratica, la sua ricerca si consolidò in una precisa posizione estetica ed ecologica. Davies motivava il suo lavoro come un atto di sensibilizzazione, dichiarando: "I invent new musical instruments for people to enjoy new experiences and to increase their sensitivity to the environment of our polluted world, as a small gesture against consumerism and the tendency to throw everything away instead of recycling the waste materials of our society" [@davies2002soundsheard, 31]. Questa visione, emersa nel clima di crisi economica e crescente coscienza ambientale degli anni Settanta in Gran Bretagna, si propose esplicitamente come correttivo allo spreco della società dei consumi. Ma per Davies, la costruzione di strumenti era solo il primo passo di un progetto sociale e pedagogico più ampio. Attraverso workshop, dimostrazioni pubbliche e attività didattiche, egli trasformava la pratica del bricolage elettroacustico in un atto di divulgazione collettiva. Tra i suoi obiettivi c'era la volontà insegnare a guardare il mondo materiale con orecchie nuove, democratizzando l’accesso alla creatività sonora. In questo senso, la sua filosofia del riuso diventava un potente strumento di emancipazione culturale: mostrare che la tecnologia musicale poteva essere smontata, compresa e reimmaginata da chiunque, al di fuori delle logiche del mercato e dell’istituzione specializzata. I suoi strumenti, quindi, non erano solo oggetti sonori, ma prototipi didattici di una possibile relazione, più consapevole e creativa, con la tecnologia e con l’ambiente.

Davies stesso definì i suoi strumenti "sintetizzatori di musique concrète", concettualizzando una significativa cortocircuitazione storica. La sua pratica portava infatti la logica fondante della *musique concrète*—l'esplorazione e manipolazione di suoni reali—direttamente nella dimensione della performance istantanea, senza la mediazione dello studio su nastro. Questo approccio democratizzava tecniche compositive un tempo confinate nelle istituzioni, rendendole accessibili attraverso l'artigianato e il riuso del domestico.

### AMM e l'etica dell'improvvisazione collettiva

La prassi improvvisativa di AMM si distinse nettamente dalle formazioni americane per la radicalità del suo approccio collettivo. A differenza del modello del "repertory ensemble", in AMM scomparve ogni distinzione tra compositore e performer, fondendo le identità in un'unica entità creativa. Keith Rowe descrisse questa pratica come una ricerca collettiva di suoni, un processo esplorativo condiviso dove l'identità del gruppo prevaleva costantemente su quelle individuali[@prevost1995]. La strumentazione rifletteva questo rifiuto: la chitarra di Rowe, suonata orizzontalmente con oggetti, le radio a onde corte di Prévost e le percussioni archettate abbandonavano consapevolmente i loro linguaggi tradizionali.

Cornelius Cardew teorizzò questa filosofia nel saggio *Towards an Ethic of Improvisation* (1971), dove tre virtù risultarono particolarmente significative. La *Semplicità* era intesa come distillazione essenziale che "must contain the memory of how hard it was to achieve"[@cardew1971]. L'*Integrità* spostava il focus dall'intenzione all'azione concreta, mentre l'*Accettazione della Morte* riconosceva la transitorietà della musica improvvisata come sua caratteristica fondamentale.

Questa etica trovò radicalizzazione nella *Scratch Orchestra*, che democratizzò il ruolo del performer permettendo a chiunque, indipendentemente dalle competenze, di partecipare[@born1995, pp. 59-60]. Il suo "Research Project" era obbligatorio per tutti i membri, senza distinzioni, e i concerti si tenevano in spazi non convenzionali, rifiutando la sacralità della sala da concerto. Cardew concepì l'orchestra come incarnazione di ideali insieme musicali, educativi e sociali.

Questa priorità sociale distinse l'approccio britannico da quello americano. Mentre negli Stati Uniti si manteneva la competenza tecnica come prerequisito, in Inghilterra si eliminò ogni barriera tecnica per un'accessibilità totale. Figure come Hugh Davies operavano proprio in questo crocevia, trasformando la complessità tecnica del live electronics in pratiche di bricolage insegnabili e trasmissibili.
Come osservato da Hugh Davies, le partiture verbali dell'epoca riducevano il controllo tradizionale del compositore a favore di un coinvolgimento più ampio[@davies2002soundsheard, 14]. La tecnologia venne così subordinata all'obiettivo di creare spazi di produzione musicale orizzontali e non gerarchici.

# IRCAM E IL MODELLO ISTITUZIONALE

## Genesi di IRCAM: dalla crisi del serialismo all'istituzione (1970–1977)

### La crisi compositiva del serialismo e la necessità di un centro di ricerca

Verso la fine degli anni Sessanta, il progetto del serialismo integrale – promessa di un rinnovamento radicale del linguaggio musicale per l'avanguardia europea postbellica – manifestava ormai un chiaro esaurimento di slancio. La sua ambizione di rifondare completamente i fondamenti della "lingua" musicale occidentale, fornendo un sistema compositivo universale paragonabile alla tonalità, era progressivamente collassata sotto il peso delle sue contraddizioni interne. Anche quella che Pierre Boulez aveva concepito come una *certezza* – definita persino "certezza sull'incertezza" riguardo ai processi aleatori – si trovava ora avvolta da un clima di profondo dubbio sul lascito del modernismo serialista [@born1995, p. 3].

La difficoltà per Boulez non era puramente estetica, ma investiva la prassi compositiva in modo sostanziale. Diversi osservatori hanno notato come la sua produzione creativa subì un netto declino dopo la metà del decennio. Il compositore francese ammise in seguito che il progetto IRCAM nacque proprio per affrontare quelli che individuava come "ostacoli fondamentali" nel proprio lavoro, ostacoli che egli considerava peraltro caratteristici dell'intero panorama compositivo contemporaneo [@born1995, p. 353, n. 11]. La via d'uscita, quindi, non poteva consistere in un semplice ripensamento individuale della tecnica, ma richiedeva la creazione di un'infrastruttura dedicata, in grado di esplorare nuovi orizzonti attraverso la ricerca scientifica e lo sviluppo tecnologico.

### Il modello "music research": scientificizzazione del processo compositivo

La visione di Boulez per IRCAM affondava le sue radici in un'idea specifica di "ricerca musicale" che aveva cominciato a formarsi nel contesto francese già durante gli anni Sessanta. Questo concetto, erede delle indagini portate avanti dal Groupe de Recherches Musicales (GRM) di Pierre Schaeffer sulla *musique concrète*, proponeva un approccio sistematico alla composizione fondato sull'analisi scientifica dei materiali sonori [@born1995, pp. 85-86]. La "ricerca musicale" si configurava così come un processo integrato, dove l'indagine acustica e psicoacustica e lo sviluppo tecnologico alimentavano direttamente la creazione compositiva, generando nuovi materiali, strutture e strumenti.

Le ambizioni di Boulez, tuttavia, miravano a superare il modello del GRM. La sua "ricerca musicale" aspirava a una razionalizzazione dell'intero "linguaggio musicale" in termini scientifici, un tentativo di scientificizzare non solo i materiali ma il processo compositivo stesso, trasformandolo da pratica individuale in un'impresa collaborativa e istituzionale. Questa visione si inseriva perfettamente nella tendenza, emersa nel serialismo del dopoguerra con figure come Boulez, Stockhausen e Babbitt, a ricorrere in modo crescente ai media elettronici e a teorie di impronta scientista. Il dialogo della musica doveva estendersi così a discipline come l'acustica, la fisica, la matematica, la teoria dell'informazione e la linguistica, chiamate a contribuire collettivamente alla rifondazione della composizione su basi rigorose.

L'occasione per tradurre questa visione in realtà istituzionale giunse in un momento politico particolare. Nel 1970, il presidente Georges Pompidou stava progettando il Centre Georges Pompidou, il nuovo museo nazionale d'arte contemporanea concepito per rifondare la cultura francese dopo la crisi del Maggio '68. Fu in questo contesto che, durante una cena all'Eliseo, Pompidou — desideroso di ricucire i rapporti con un Boulez ormai da quasi un decennio in una sorta di esilio volontario all'estero e in contrasto con l'establishment culturale parigino — offrì al compositore una *carte blanche* per realizzare il centro di ricerca musicale a cui questi pensava da anni. L'invito era esplicito: partecipare al rinnovamento della vita artistica nazionale progettando di fatto l'"ala musicale" del nuovo centro.

Quell'incontro rappresentò la convergenza di due visioni distinte ma complementari. Da un lato, Pompidou perseguiva una politica culturale basata su grandi istituzioni centralizzate, proiettate verso il futuro ma anche capaci di attrarre un pubblico ampio, in un'ideologia che cercava di conciliare centralismo, arte d'élite e una certa dose di populismo. Dall'altro, Boulez vedeva nell'istituzionalizzazione della ricerca musicale l'unica via praticabile per superare lo stallo creativo in cui versava l'avanguardia serialista. Il risultato fu la decisione di affidare direttamente a Boulez il potere di progettare e dirigere il futuro istituto, una scelta che rifletteva il ruolo prominente che gli intellettuali francesi hanno tradizionalmente rivendicato nella sfera pubblica e nelle cariche di governo.

### Pianificazione, costruzione e status giuridico (1970–1977)

Il periodo tra il 1970 e il 1977 fu dedicato interamente alla pianificazione, costruzione e sviluppo istituzionale di IRCAM. Una tappa fondamentale fu raggiunta nel 1977, quando l'istituto ottenne uno status giuridico peculiare: da dipartimento musicale pubblico del Centre Pompidou, divenne un'associazione privata semi-autonoma, pur mantenendo legami formali con l'ente madre. Questa autonomia, giustificata dalla missione di "produzione" e "originazione creativa" attribuita a IRCAM – in netto contrasto con il ruolo di "riproduzione" culturale di musei e teatri – gli conferiva una flessibilità operativa unica. Consentiva infatti una gestione finanziaria più agile, la possibilità di assumere ricercatori stranieri e di accettare finanziamenti privati, prerogative precluse ad altre istituzioni culturali statali francesi.

L'inaugurazione ufficiale nel 1977 fu sancita dalla monumentale serie di concerti *Passage du Vingtième Siècle*, un evento senza precedenti per scala e ambizione nella storia della musica contemporanea. In circa settanta concerti distribuiti nell'arco di un anno a Parigi, furono presentate le opere di oltre cento compositori moderni. Questo colossale progetto inaugurale rappresentava una dichiarazione fondativa, il tentativo di conquistare per la Francia, sotto la guida del ritornato "figliol prodigo" Boulez, un ruolo centrale e legittimante per il futuro della musica.

### Un modello alternativo: razionalizzazione e centralizzazione

Con IRCAM si consolidava un modello radicalmente diverso rispetto alle coeve esperienze di live electronics che fiorivano negli Stati Uniti o in Gran Bretagna. Mentre compositori come Gordon Mumma, David Tudor o Alvin Lucier sviluppavano pratiche fondate sull'autocostruzione, su collaborazioni orizzontali e sulla portabilità della tecnologia, l'istituto parigino incarnava una visione opposta: la trasformazione del lavoro creativo da attività individuale in processo istituzionale, razionalizzato all'interno di una complessa divisione del lavoro che integrava composizione, ricerca scientifica e sviluppo tecnologico [@born1995, pp. 85-86].

Questa scelta rappresentava l'incarnazione di una tendenza più ampia verso la centralizzazione e la razionalizzazione, destinata a manifestarsi su molteplici piani. La centralizzazione era infatti istituzionale e amministrativa, ma si rifletteva anche nei contenuti estetici promossi, focalizzati sul contributo della scienza alla composizione, nelle pratiche di produzione tecnologica e nella stessa organizzazione interna del lavoro. L'approccio arrivava persino a coinvolgere, in una logica di gestione della "domanda", strategie di marketing e indagini di mercato.

Se in centri come Ann Arbor, Londra o Tokyo si sviluppavano ecosistemi basati sul fai-da-te, la cooperazione e una filosofia della necessità, Parigi stava costruendo il più grande centro al mondo dedicato alla musica contemporanea e computerizzata. IRCAM rappresentava così, attraverso l'istituzionalizzazione della creatività stessa, una nuova e decisiva tappa nell'istituzionalizzazione della musica.

## Centralizzazione tecnologica e "grandi macchine"

### Il 4X: tecnologia real-time e sintesi digitale

Nel 1975, il compositore Luciano Berio, allora co-direttore di IRCAM, coinvolse il fisico Giuseppe Di Giugno in un progetto ambizioso: realizzare un sintetizzatore in tempo reale dotato di "mille oscillatori" [@chadabe1997, p. 180]. La richiesta, apparentemente impossibile, rispondeva all'esigenza di superare i limiti tecnologici degli studi elettroacustici precedenti, come lo Studio di Fonologia di Milano, e di recuperare la possibilità di una sintesi sonora immediata che i primi computer digitali avevano reso più lenta e complessa.

Lo sviluppo procedette attraverso una serie di prototipi. Dal primo modello 4A (1976), che consentiva di sommare forme d'onda ma senza interazione tra gli oscillatori software, si passò al più avanzato 4B (1977) con sessantaquattro oscillatori interconnessi. La svolta concettuale arrivò con il 4C (1979), che abbandonò il paradigma dell'oscillatore fisso in favore di "oggetti" algoritmici programmabili e interconnessi [@chadabe1997, p. 181]. Questa evoluzione rifletteva la filosofia di Di Giugno, secondo cui i sintetizzatori dovevano essere costruiti per le esigenze dei musicisti, non per la comodità dei loro progettisti, perseguendo quindi massima generalità e flessibilità d'uso.

Il sistema definitivo, il 4X, fu presentato pubblicamente nel 1981 durante la prima esecuzione di *Répons* di Pierre Boulez. La macchina univa capacità di sintesi e analisi in tempo reale, supportando tecniche diversissime come sintesi additiva e FM, campionamento, filtraggio numerico e una vasta gamma di effetti. La sua potenza lo rese uno strumento unico nel panorama dell'epoca, sebbene l'avvento di sistemi commerciali come il Lucasfilm ASP iniziasse presto a minacciarne il primato tecnico.

Una criticità del progetto risiedeva nel divario tra l'hardware potente e la carenza di software e periferiche necessari a renderlo pienamente utilizzabile. Questo squilibrio, dovuto anche a differenze di priorità tra i progettisti, ritardò di anni la realizzazione di una vera e propria "workstation musicale" completa. I compositori che ne trassero maggiore vantaggio furono pertanto quelli strettamente affiliati a IRCAM, come Boulez e i ricercatori residenti. Per costoro, il 4X incarnava un esperimento utopico: uno strumento che, grazie alla sua risposta istantanea e alla futura integrazione con controller gestuali, poteva favorire un metodo compositivo empirico e reattivo attraverso una tecnologia in *real-time*.

### Esclusività tecnologica e contraddizioni produttive

Tuttavia, la storia del 4X rivela dinamiche istituzionali che andavano ben oltre le questioni puramente musicali. Verso la fine del 1983 e l'inizio del 1984, si svolsero negoziati prolungati con un'azienda chiamata Sogitec, che produceva componenti per aerei ed era strettamente legata all'industria della difesa [@born1995, pp. 109-110]. Significativamente, Sogitec non era interessata alle capacità musicali del 4X: il direttore dell'industrializzazione vendette la macchina trovando un modo di usarla per simulare il rumore degli aerei, e l'azienda la acquistò per diventare la base di un simulatore di rumore di volo [@born1995, p. 110] (interessante notare come le realtà sociali americane o inglesi, che usavano materiali e componenti elettronici ereditati dalla guerra e in disuso, sono esattamente agli antipodi con quella che si può definire a tutti gli effetti un'industria culturale privata che vive di fondi istituzionali e rapporti l'industria bellica. Da un lato rivalorizzare restituendo un futuro ad elementi di scarto, dall'altro prestare servizi per la guerra in cambio di una manciata di oscillatori). Nel luglio 1984, Sogitec fu improvvisamente rilevata dal gigante della difesa Dassault — produttore di aerei e armi high-tech — il che ritardò ulteriormente i tempi di produzione [@born1995, p. 110].

Alla fine, IRCAM concesse a Sogitec i diritti sul prototipo del 4X per avviarne una produzione industriale, ricevendo in cambio solo quattro unità complete e una royalty minima [@born1995, p. 110]. Sebbene il sistema fosse così valorizzato come tecnologia utile, l'azienda Dassault/Sogitec – priva di interesse per il mercato musicale – non lo commercializzò mai come sintetizzatore, né ne produsse un numero sufficiente per una distribuzione ad altri centri. Il suo costo, stimato intorno ai 100.000 dollari [@chadabe1997, p. 183], ne limitava drasticamente l’accessibilità al di fuori di contesti molto specializzati.

La vicenda del trasferimento tecnologico a Sogitec fu oggetto di riserbo all'interno di IRCAM nel 1984. Solo pochi dipendenti accennarono, in confidenza, al proprio disagio per le implicazioni militariste dell'accordo e per il fallimento nel diffondere il 4X a un pubblico più ampio. Il progettista principale, BU, reagì con rabbia al passaggio e si rifiutò di consegnare alla società la documentazione scritta del progetto, costringendola a decifrarne il funzionamento da zero – un fattore che causò ritardi tali da far arrivare le unità appena in tempo per le prime esecuzioni di *Répons*.

Questo rifiuto della documentazione scritta si ricollegava a un principio fondativo di IRCAM, promosso da Luciano Berio: l'idea dell'istituto come una "cultura orale", dove la conoscenza tecnica e musicale si trasmetteva direttamente tra persone, privilegiando il suono rispetto al testo [@born1995, p. 235]. Quel che era nato come principio utopico e anarchico divenne, nel caso del 4X, una forma di segretezza che alimentava un alone mitico e quasi feticistico attorno alla macchina.

IRCAM si trovava quindi ad abitare una contraddizione profonda. Il suo statuto la impegnava a sviluppare tecnologie innovative, di tipo non commerciale, eppure il loro scarso interesse per il mercato privato le relegava a uno status di prototipi artigianali, prodotti in numeri esigui e con una circolazione minima. Ciò ne limitava l'influenza e il valore economico diretto. Tuttavia, questa stessa mancata commercializzazione preservava il massimo valore simbolico della tecnologia. La sua esclusività, confinata alle mura dell'istituto, conferiva un prestigio unico a opere come *Répons* e manteneva a IRCAM un'aura di indispensabilità, offrendo ai compositori un accesso a strumenti irripetibili altrove.

### Mediazioni tecnologiche e complessità del software

Il 4X rappresentava solo una componente, per quanto centrale, dell'architettura tecnologica di IRCAM. Parallelamente allo sviluppo hardware, l'istituto stava costruendo un complesso ecosistema software che introduceva forme di mediazione radicalmente diverse rispetto alle pratiche del live electronics americano fondate sul fai-da-te. Se compositori come Mumma e Tudor operavano attraverso la manipolazione diretta e tangibile dei circuiti, a IRCAM si sviluppava un modello basato su una gerarchia verticale di codici software, che moltiplicava le traduzioni necessarie tra l'intenzione compositiva e il risultato sonoro [@born1995, pp. 226-232].

Questa "multitestualità" era una caratteristica intrinseca del medium software. Il suo utilizzo comportava una scrittura di istruzioni all'interno di una stratificazione di linguaggi, ciascuno dei quali doveva essere tradotto nel livello inferiore senza perdita di significato operativo. La gerarchia tipica partiva dal codice macchina binario, saliva attraverso l'assembler e il sistema operativo, per arrivare ai linguaggi di programmazione ad alto livello come C o LISP. I linguaggi per la computer music, come quelli sviluppati a IRCAM, aggiungevano un ulteriore strato, essendo essi stessi costruiti su questi linguaggi generali.

Questa stratificazione aveva implicazioni profonde per la pratica compositiva. Ogni layer costituiva una distanza aggiuntiva dal controllo sonoro diretto, trasformando la composizione in una forma di programmazione multilivello. La densità delle mediazioni tecnologiche e codificate era quindi molto maggiore a IRCAM che nelle musiche tradizionali o nelle tecniche empiriche dell'elettronica analogica [@born1995, p. 231]. Tale struttura gerarchica creava inoltre una netta distinzione tra utenti. I compositori non esperti potevano interagire solo con il livello superficiale del codice, restando impotenti di fronte a malfunzionamenti o limitazioni del sistema, incapaci di scendere negli strati più profondi per comprenderli o modificarli. In questo modo, il modello software di IRCAM istituzionalizzava non solo la ricerca, ma anche una specifica e asimmetrica relazione di potere tra il compositore e lo strumento tecnologico.

## Répons (1981): case study del metodo IRCAM

### Concezione dell'opera e sviluppo simbiotico con il 4X

Répons fu concepito fin dall'inizio non semplicemente come opera musicale, ma come dimostrazione delle capacità tecnologiche di IRCAM. Come osserva Bernardini, l'opera "era destinata a mostrare sia la virtuosità dell'Ensemble Intercontemporain che lo stato dell'arte tecnologico di IRCAM" [@bernardini1986]. La genesi dell'opera fu intimamente legata allo sviluppo del 4X stesso: storicamente, il progetto del 4X fu "strettamente coinvolto" (closely implicated) proprio con Répons di Boulez [@born1995, p. 140]. Questo sviluppo "simbiotico" significava che le esigenze compositive di Boulez guidavano direttamente l'evoluzione tecnologica della macchina, mentre le possibilità emergenti del 4X aprivano nuove direzioni compositive.
L'organico dell'opera prevedeva ventiquattro musicisti e sei solisti — pianoforte, sintetizzatore, arpa, cimbalom, vibrafono e xilofono — i cui suoni venivano trasformati indipendentemente e diffusi attraverso diversi altoparlanti disposti intorno alla sala [@chadabe1997, p. 182]. La struttura drammaturgica era concepita teatralmente: dopo una parte introduttiva strumentale brillante, l'ingresso teatrale dei solisti portava con sé una serie di trasformazioni live electronics che generavano echi trasformati delle parti dei solisti nello spazio [@bernardini1986]. Si trattava di una moltiplicazione spaziale e timbrica dei gesti solistici, dove il 4X agiva come mediatore tra l'azione strumentale e la sua proiezione trasformata nell'ambiente acustico.

### Division of labor: il ruolo del tutor e la questione dell'authorship

Tuttavia, la realizzazione di Répons non fu opera di Boulez da solo, ma il risultato di una collaborazione prolungata con il suo tutor non ufficiale, BYV, che divenne direttore del 4X Software [@born1995, pp. 264-268]. Questa collaborazione rivela in modo esemplare il modello IRCAM di divisione del lavoro tra compositore e tecnologo — una divisione che, come vedremo, solleva questioni profonde sull'authorship musicale.
BYV descrive così il rapporto di lavoro con Boulez:

> "Quando abbiamo iniziato a lavorare insieme, l'ho fatto lavorare sulla macchina. Ma sai, è come uno strumento, devi esercitarti regolarmente. E Pierre era troppo irregolare, quindi i suoi progressi non erano davvero così [buoni]... Fondamentalmente lavoro per lui per un sentimento positivo. Altrimenti, non lavorerei con altri compositori; non sarei un tutor che passa da un universo musicale all'altro, di qualità variabile. Ciò che trovo soddisfacente nel lavoro con Pierre è il senso di continuità nel tempo" [@born1995, pp. 267-268].

Il lavoro di BYV non era semplicemente "assistenza tecnica" ma comportava decisioni musicalmente rilevanti. Come egli stesso ammette, "le soluzioni su cui lavoro con lui porteranno ovviamente in una certa misura parti della mia personalità" [@born1995, p. 268]. Tuttavia, BYV mantiene fermamente che "l'autore indiscusso di Répons o di qualsiasi pezzo su cui lavoro con Pierre è Pierre stesso, non c'è dubbio" [@born1995, p. 268]. Questa affermazione solleva però interrogativi: se BYV è responsabile della concezione e manipolazione della tecnologia, della produzione dei materiali sonori nuovi, il suo contributo all'authorship complessiva del pezzo non è forse "considerevole"? [@born1995, p. 267]
Il modello di lavoro era questo: Boulez dava a BYV "un compito piccolo o grande, e quando tornava ascoltava il risultato" [@born1995, p. 267]. BYV lavorava quindi autonomamente sulla macchina durante le assenze di Boulez — che erano frequenti data la sua carriera di direttore d'orchestra — sviluppando soluzioni tecniche che incorporassero le indicazioni musicali ricevute. Era una collaborazione irregolare ma continuativa, che si estendeva su un arco temporale lungo (quasi cinque anni al momento dell'intervista di Born) e che includeva non solo Répons ma "altri progetti all'orizzonte" e "lavoro su problemi che vanno oltre i pezzi particolari" [@born1995, p. 268].

### Dipendenze sistemiche e problemi di processo

La preparazione per la prima esecuzione parigina di *Répons* nell'ottobre 1984 fu un percorso irto di difficoltà tecnologiche, che impegnò interi team nella risoluzione di problemi sia hardware che software. L'opera, nella quale il sistema 4X aveva un ruolo protagonista, dipendeva dalla produzione industriale di un numero sufficiente di queste complesse macchine, una negoziazione che si rivelò tutt'altro che semplice.

L'intera infrastruttura informatica dell'istituto viveva in quel periodo una fase di profonda instabilità. La recente transizione a un nuovo sistema basato su DEC VAX e sul sistema operativo UNIX, sebbene allineata con gli standard di ricerca internazionali, era di per sé fonte di continue incertezze. La decisione di aggiornare alla versione 4.1a di UNIX, necessaria per connettersi a nuove reti e gestire le future workstation "Valids" collegate al 4X, costrinse a una massiccia riscrittura del codice di base, generando un'ondata di bug che richiese settimane di meticoloso lavoro di debugging per ristabilire un funzionamento normale [@born1995, pp. 104-105, 257].

Le note etnografiche di Georgina Born documentano come sessioni di lavoro quotidiane fossero dominate non dalla scrittura musicale, ma dalla caccia a malfunzionamenti spesso bizzarri, come misuratori di ampiezza che fluttuavano selvaggiamente in assenza di qualsiasi suono. In questo contesto, l'amara battuta di un compositore in residenza – secondo cui la computer music rappresentava la forma più alta di esperienza umana perché costringeva a godere del puro processo lavorativo, rinunciando alla gratificazione del risultato finale – catturava l'essenza di una frustrazione diffusa. La lotta con la tecnologia rischiava costantemente di soffocare la creazione musicale stessa, presentando una visione del "vivere il processo" radicalmente diversa dallo sperimentalismo empirico americano.

### La première come showcase istituzionale

La prima esecuzione parigina di *Répons* nell'ottobre 1984 fu concepita come un evento istituzionale di massimo rilievo. Presentata in una sala appositamente allestita del Centre Pompidou per sei serate consecutive al completo, l'opera mirava a mostrare al pubblico nazionale d'élite e alla comunità internazionale della computer music riunita per l'ICMC il vertice delle capacità musicali e tecnologiche di IRCAM. La presenza di figure politiche di primo piano nella prima fila sottolineava il significato anche simbolico-politico dell'evento.

Questa doppia platea – nazionale e internazionale, politica e scientifica – caricava la prima di una funzione di legittimazione multipla: per l'istituto IRCAM nel suo complesso, per la tecnologia esclusiva del 4X e per l'approccio compositivo di Boulez. Tale pressione istituzionale amplificava però la fragilità intrinseca del modello. Il sistema 4X, consegnato da Sogitec con ritardo a ridosso della prima a causa di problemi di documentazione e produzione, introdusse una precarietà tecnica dell'ultimo minuto. Questo aspetto, pur avvicinandosi superficialmente alle situazioni di instabilità che caratterizzavano il lavoro di un artista come David Tudor, emergeva qui in un contesto totalmente differente e da presupposti opposti: in Tudor, l'aleatorietà del circuito e il rischio del malfunzionamento erano una **caratteristica poetica intenzionale**, parte integrante di un'estetica che abbracciava il caso e il comportamento imprevedibile dell'hardware; a IRCAM, l'instabilità era invece l'effetto collaterale indesiderato di un sistema chiuso, estremamente complesso e dipendente da una lunga catena di produzione e mediazione tecnologica.

*Répons* si confermò così come l'incarnazione perfetta e paradossale del metodo IRCAM: unendo ambizione compositiva monumentale, sviluppo tecnologico su misura e una cronica instabilità di sistema, l'opera funzionava sia come traguardo creativo che come strumento di legittimazione per l'intero progetto istituzionale.

## Limiti del modello: ricerca fondamentale vs produzione musicale

### Tensione research vs production

Nel 1984, la contraddizione più profonda del modello IRCAM emerse attraverso un gruppo informale di ricercatori noto come il "musicians' group". Questo collettivo, che rappresentava l'élite intellettuale e tecnica dell'istituto, iniziò a riunirsi per discutere il futuro della ricerca musicale, rivelando una frattura insanabile tra due visioni opposte della missione istituzionale [@born1995, pp. 133-134, 194-221].
Da una parte, il gruppo sosteneva la necessità di una ricerca fondamentale a lungo termine, indipendente dalle esigenze immediate della produzione musicale. Il loro modello ideale era una forma di "ricerca pura", un processo aperto e continuo di sperimentazione il cui valore risiedeva nella ricerca stessa, non in applicazioni immediate. Dall'altra, la direzione di Boulez e del Direttore Scientifico insisteva sul legame indissolubile tra ricerca e produzione, richiedendo che la prima si traducesse in risultati concreti – sia musicali che tecnologici – entro tempi definiti e brevi.
Questa opposizione tra ricerca aperta e produzione finalizzata non era solo terminologica, ma rifletteva una crisi epistemologica ed operativa. Come sintetizzò amaramente un junior tutor del gruppo, la frattura nasceva da una fondamentale incomprensione del processo creativo tecnologico:

> "L'ala di produzione musicale là, Boulez e [il Direttore Artistico], si rifiutano di capire quale sia questo processo perché non sono stati lì dentro a lottare con lo sviluppo loro stessi. Non vogliono sapere di tutta quella 'spazzatura' [di ricerca], solo del risultato musicale finale. C'è una sorta di impazienza, e capisco. Ma so che per arrivarci, devi passare attraverso certi passaggi" [@born1995, pp. 214-215].

Il conflitto metteva così a nudo la tensione irrisolta tra due anime di IRCAM: quella dell'istituto di ricerca pura e quella della fabbrica di produzione musicale d'avanguardia.

E il problema non era astratto: impattava direttamente sulla produzione musicale. Nel 1984, quello che doveva essere un anno di intensa attività compositiva si rivelò invece "eccezionalmente improduttivo" [@born1995, p. 105]. Erano state pianificate quattro commissioni di compositori visitatori, ma solo tre ebbero luogo, e una di queste non risultò in un pezzo [@born1995, p. 105].
L'obiettivo dal 1985 era avere dodici compositori visitatori all'anno [@born1995, p. 105] — un target che rivelava quanto la produzione musicale effettiva fosse ben al di sotto delle aspettative.
La tensione tra ricerca e produzione creava un circolo vizioso: i ricercatori avevano bisogno di tempo per sviluppare sistemi musicalmente potenti, ma i compositori visitatori avevano bisogno di strumenti stabili e documentati per produrre musica nei loro tre mesi di permanenza.

La tensione all'interno di IRCAM si manifestò concretamente nel dibattito sui "small systems", come il Macintosh e lo Yamaha DX7. Nonostante un accordo per ottenere alcuni di questi computer in cambio di software, la loro introduzione incontrò una feroce opposizione, simbolicamente sintetizzata dal netto rifiuto di Pierre Boulez. Anche il musicians' group era diviso sulla questione, poiché questi sistemi accessibili e commerciali minacciavano il fondamento stesso dell'istituto, basato su tecnologie esclusive e su misura [@born1995, pp. 283-284].

Il conflitto si intrecciava alla crescente disillusione di Boulez verso il concetto di "ricerca musicale" da lui stesso promosso. Egli rimproverava sempre più spesso i ricercatori di essere troppo astratti e di non orientare il lavoro verso risultati produttivi tangibili e a breve termine, insistendo per una stretta integrazione tra sperimentazione e produzione musicale concreta. Questa ambivalenza – tra la retorica dell'utopia della ricerca e l'impazienza per i risultati – divenne proverbiale tra i collaboratori e fu persino oggetto di satira interna, culminando in una falsa voce bibliografica che beffeggiava lo scetticismo ormai diffuso verso l'intero progetto [@born1995, p. 216]. Il dibattito sui small systems rivelava così, in microcosmo, la crisi d'identità di IRCAM, diviso tra la missione di un centro di ricerca pura e le pressioni per funzionare come una fabbrica di produzione musicale d'avanguardia.

### Collasso sociale

Il modello IRCAM si rivelò insostenibile a causa di profonde contraddizioni strutturali. Coloro che incarna l'ideale di ricerca musicale pura – tutor e ricercatori – godevano di un alto status culturale all'interno dell'istituto, ma erano allo stesso tempo la forza lavoro più precaria e moderatamente retribuita, un paradosso che minava la stabilità del sistema. Inoltre, l'enorme investimento in tecnologie esclusive e complesse, come il 4X, non si traduceva in una corrispondente produttività musicale; al contrario, creava un collo di bottiglia creativo, rendendo possibile solo poche opere monumentali come *Répons* e rendendo i compositori totalmente dipendenti da una macchina instabile e da un supporto tecnico specializzato. Questo approccio centralizzato e mediato istituzionalmente contrastava radicalmente con il modello DIY americano, fondato su tecnologie accessibili, modificabili e performabili in tempo reale. Il risultato fu un sistema che, per molti versi, aveva prodotto più un costoso oggetto di prestigio che uno strumento musicale funzionale e diffusibile.

# CONCLUSIONI

L’indagine storica condotta in questa tesina conferma l’ipotesi di partenza: il live electronics non costituisce un capitolo unitario della storia della musica, ma un campo di forze plurali, in cui pratiche, filosofie e modelli organizzativi radicalmente differenti hanno risposto, in modi altrettanto diversi, a un’esigenza comune. Tale esigenza, emersa dalla crisi della *tape music*, non fu meramente tecnica – uscire dallo studio – ma profondamente esistenziale e sociale: riconquistare per la musica elettroacustica quella dimensione irripetibile dell’evento, il *hic et nunc* performativo, che ne costituisce l’essenza viva.

Il percorso analitico, muovendo dalla fondamentale distinzione tra *live* e *real-time*, ha rivelato come la risposta a questa esigenza si sia articolata lungo due assi divergenti. Da un lato, il modello istituzionale ed europeo, esemplificato da IRCAM, ha cercato di ricostruire la liveness dall’alto, attraverso un’impresa collettiva, razionalizzata e iper-tecnologica. Se da un punto di vista ingegneristico questo approccio ha prodotto strumenti di straordinaria potenza, dal punto di vista performativo ed artistico esso ha spesso tradito la sua stessa missione. La complessa divisione del lavoro, la mediazione software gerarchica, la dipendenza da “grandi macchine” instabili e l’inevitabile burocratizzazione del processo creativo hanno finito per diluire, quando non abbattere, il senso stesso dell’agire artistico. In questo modello, la tecnologia rischia di diventare fine a se stessa, e l’evento dal vivo si trasforma nello *showcase* di un apparato, piuttosto che nell’espressione immediata di una presenza condivisa.

Dall’altro lato, il modello DIY e comunitario sviluppatosi principalmente negli Stati Uniti – e declinato in forme peculiari anche in contesti come quello britannico – ha perseguito la liveness dal basso, attraverso la pratica materiale, l’autocostruzione, la condivisione orizzontale della conoscenza e l’accettazione dell’imperfezione. È in questo contesto che il principio della *liveness* trova la sua espressione più autentica e feconda. Il circuito autocostruito con surplus militare, il microfono a contatto su un oggetto quotidiano, la partitura come sistema aperto o come diagramma da esplorare: sono tutti elementi di un dispositivo di comunità, prima che tecnico. Qui, la precarietà del mezzo non è un limite da nascondere, ma la condizione stessa di un’esplorazione collettiva e imprevedibile. La performance diventa il luogo di una ricerca comune, in cui il suono si genera nell’interazione immediata tra persone, strumenti e spazio. In questa luce, il live electronics DIY non è semplicemente un modo per fare musica: è una pratica sociale e, in senso ampio, politica, che afferma la possibilità di costruire rapporti creativi al di fuori dei circuiti istituzionali standardizzati, valorizzando l’accessibilità, la cooperazione e l’agency diretta del performer.

Alla domanda iniziale – *in risposta a cosa* è nato il live electronics – possiamo quindi rispondere: è nato in risposta a un desiderio di riconnessione. Riconnessione del suono con il gesto che lo genera, della tecnologia con il corpo che la manipola, del compositore con il performer e con il pubblico all’interno di uno spazio condiviso. La storia qui raccontata è, volontariamente, una storia al plurale. Non esiste una linea di progresso che da Cage conduca a IRCAM, o viceversa. Esistono invece storie parallele, a volte intrecciate, spesso antagoniste, che testimoniano come la stessa esigenza artistica possa dare vita a ecosistemi culturali opposti.

La lezione che questo racconto plurale offre al presente è più che mai attuale. In un panorama musicale dominato da tecnologie digitali sempre più standardizzate, omogenee e “opache”, l’eredità del live electronics DIY ci ricorda che la tecnologia è sempre una scelta culturale e, quindi, politica. Scegliere strumenti modificabili, pratiche cooperative e un’estetica della scoperta piuttosto che del controllo, significa preservare uno spazio per una liveness autentica: non la perfetta esecuzione di un programma, ma l’apertura imprevedibile di un evento in cui, come avrebbe detto David Tudor, è lo strumento stesso a “insegnarci cosa vuole ascoltare”. La sfida futura non sarà forse quella di costruire macchine più potenti, ma di progettare, collettivamente, le condizioni per cui la tecnologia serva, e non soffochi, questa irriducibile vita del suono.

# BIBLIOGRAFIA

::: {#refs-bib}
:::

# SITOGRAFIA

::: {#refs-sit}
:::

