---
abstract: |
  questo dovrebbe essere il sommario...
---

# INTRODUZIONE
## Presentazione tema e domanda centrale
## Metodologia: analisi fonti primarie
## Struttura della tesi


# GENESI DEL LIVE ELECTRONICS

## "Live" vs "Real-time" - Una distinzione necessaria

Prima di approfondire l'analisi e dei contesti e delle prassi musicali, è essenziale chiarire una distinzione terminologica che percorrerà l'intera ricerca. In questo paragrafo, mi riferirò alle teorie della *liveness* messe in luce da pensatori come Agostino Di Scipio per sciogliere un equivoco fondamentale: i termini *live* e *real-time*, spesso usati come sinonimi, designano in realtà dimensioni profondamente differenti della pratica musicale elettroacustica.

Nella letteratura specialistica, a partire dagli anni Settanta, si è affermata una tendenza problematica, rilevata da Di Scipio, in cui le istituzioni dedicate alla computer music hanno sistematicamente preferito parlare di "computer in tempo reale" piuttosto che di "live electronics". Questa scelta terminologica privilegiava un criterio meramente tecnologico, legato alla sempre maggiore velocità computazionale, per descrivere risorse performative, introducendo un equivoco concettuale che merita chiarimento [@discipio2021thinking, 177].

La categoria del *real-time* appartiene propriamente al dominio dell'informatica e designa una capacità puramente tecnica dei sistemi: quella di elaborare dati con latenze così ridotte da risultare impercettibili. È dunque una caratteristica quantificabile, legata all'hardware e al software. Al contrario, la dimensione del "dal vivo" riguarda un ordine di fenomeni completamente diverso, legato alle condizioni irriducibili dell'evento performativo stesso. Il live electronics si definisce non attraverso parametri di latenza, ma attraverso la generazione e manipolazione contestuale del suono, mediante l'azione diretta dei performer in uno spazio condiviso con il pubblico.

Questa distinzione non è un mero tecnicismo, ma possiede implicazioni epistemologiche profonde. Come sottolinea Di Scipio, il tempo reale non è di per sé un criterio sufficiente per la *liveness*, ma deve essere integrato con una nozione di spazio reale. Ciò che definisce autenticamente l'esperienza performativa è "the indivisibility of time-space coordinates in lived experience, in fact often evoked by the common language expression the *here and now* — moulded after the Latin *hic et nunc*" [@discipio2021thinking, 177]. Una performance dal vivo si realizza all'interno di un'ecologia più ampia di azioni e percezioni, inscindibile da uno spazio fisico specifico e dalle sue connotazioni materiali e culturali.

La storia della prassi conferma questa separazione concettuale. Il live electronics, infatti, esisteva già prima dello sviluppo di computer ad alta velocità: i sistemi analogici degli anni Sessanta operavano su scale temporali diverse dagli standard odierni, ma costituivano performance autenticamente *live* perché generavano suoni nell'immediatezza dell'esecuzione. Allo stesso modo, processori digitali in grado di operare in *real-time* possono essere impiegati in produzioni in studio prive di qualsiasi dimensione performativa. Ciò che è decisivo, quindi, non è la velocità tecnica, ma la modalità d'uso: la presenza corporea, la condivisione dello spazio acustico e l'interazione con l'irripetibile contingenza del concerto.

La stessa definizione di *live electronics* incontra ulteriori problematiche. Come nota Di Scipio, la gestione di mezzi elettroacustici in concerto non nasce certo negli anni Sessanta, dato che i primi strumenti elettronici del Novecento erano concepiti per un uso dal vivo. L'espressione rappresenta piuttosto il tentativo di riunire sotto un'unica denominazione una molteplicità eterogenea di pratiche esecutive sviluppatesi in contesti geografici e istituzionali molto differenziati nel corso di decenni.

Sebbene la storiografia individui spesso in opere come *Cartridge Music* (1960) e *Mikrophonie* (1964) la nascita codificata di questa prassi, nei prossimi paragrafi ci soffermeremo sugli aspetti che ne hanno preparato il terreno, a cominciare dalla crisi della *tape music*.



## La crisi della tape music (1950-1960)

Per comprendere la portata rivoluzionaria di opere come *Cartridge Music*, è necessario ricostruire il contesto da cui emergono. Durante tutti gli anni '50, la musica elettroacustica fu quasi esclusivamente *tape music* — musica fissata su nastro magnetico e composta attraverso tecniche di editing in studio.
Come osserva Gordon Mumma:

> "During the 1950s most composers treated magnetic tape in a 'hands-on' manner analogous to that of filmmakers. As with film, tape music was composed largely through editing."[@mumma1975, p. 292]

Il nastro magnetico era "il primo supporto di registrazione per il suono che fosse ragionevolmente editabile: poteva essere tagliato e giuntato con precisione"[@mumma1975, 292].
Questa modalità di lavoro confinava i compositori in dei veri e propri laboratori con macchinari particolarmente ingombranti. Fino al 1960 vi erano pochissime eccezioni all'uso del nastro magnetico come medium esclusivamente da studio [@mumma1975, 292]. Le opere risultanti venivano poi presentate al pubblico tramite diffusione radiofonica o in concerto attraverso altoparlanti — una situazione che creava evidenti tensioni con le aspettative del pubblico da concerto.

### Il rituale del concerto

Il problema non era meramente tecnico o estetico, ma toccava il cuore stesso della presentazione musicale. I compositori che lavoravano con nastro magnetico si trovavano continuamente a sperimentare modalità per presentare le proprie opere al pubblico [@mumma1975, p. 294]. La radiodiffusione e le registrazioni domestiche funzionavano bene perché permettevano all'ascoltatore di determinare autonomamente il grado di formalità con cui ascoltare. Ma la presentazione dal vivo poneva sfide del tutto diverse: il pubblico da concerto portava con sé aspettative tradizionali consolidate: "Audiences expect to see as well as hear a performance, and loudspeakers aren't much to look at."[@mumma1975, p. 294]

Questa tensione tra il medium acusmatico — il suono privo di fonte visibile — e le convenzioni del rituale concertistico tradizionale si inseriva, come ha osservato Nicola Bernardini, in una crisi più generale del rituale del concerto stesso [@bernardini1986, 62]. L'introduzione dell'elettronica dal vivo coincise, specialmente negli Stati Uniti e in Giappone, con un momento in cui il rituale del concerto veniva rivisto, modificato, distrutto, de-strutturato praticamente per ogni nuovo brano [@bernardini1986, 62].[^1]


[^1]: Non a caso le esperienze di Cage in ambito elettroacustico, come vedremo sotto, sono inscrivibili a pratiche multi-mediali spesso con teatro, danza e luci in un panorama di sperimentazione dove la musica assolve il ruolo di un unico senso, affidando il resto alle altre arti.
vedi sezione 1.boh.

I fattori scatenanti di questa crisi erano interconnessi [@bernardini1986, 62]: l'enorme influenza di John Cage sui suoi contemporanei (paragonabile all'influenza della scuola di Darmstadt); il progressivo indebolimento del linguaggio musicale, ormai giunto ai minimi termini, che metteva in crisi la stessa legittimità del compositore; l'assenza di una prassi strumentale codificata per gli strumenti elettronici, che aprì la strada a una partecipazione del pubblico molto più attiva; l'affermarsi dell'improvvisazione libera.

In questo contesto di trasformazione radicale, numerosi compositori cercarono alternative alla presentazione tradizionale di musica su nastro, sperimentando forme di live electronics che potessero ri-animare [@discipio2021thinking] la dimensione performativa della musica elettroacustica e rispondere alle aspettative di presenza e visibilità del pubblico.

### Uscire dallo studio

Joel Chadabe apre il capitolo "Out of the Studios" del suo *Electric Sound* con una domanda e una risposta emblematiche:

> "Question: 'How do you perform electronic music without tape?' Answer (said with a shrug of the shoulders and rising inflection): 'Take away the tape.'"[@chadabe1997, p. 81]

Questa necessità di "portare la musica elettronica fuori dallo studio"[@chadabe1997, p. 81] viene osservata anche da Gordon Mumma che in un articolo passa in rassegna quelle che oggi definiremmo "best practices" per uno studio modulare, con strumentazione trasferibile dal vivo per le perfomances, è sentita anche da Stockhausen. Egli in una conferenza del 1972, definisce *Mikrophonie I* come "electronic live music" in opposizione alla "electronic music which is produced in a studio"[@roth2023, 64, citando Cott & Stockhausen 1973]. Cage, parlando di *Cartridge Music* nel 1962, aveva coniato l'espressione "live electronic music" — probabilmente per la prima volta in assoluto, come nota Di Scipio [@discipio2021, Parte Terza].
L'inversione dei termini ("live electronic" vs "electronic live") probabilmente è sintomatica di approcci di lavoro e radici culturali differenti (l'idea di musica elettronica per un compositore come Stockhausen proveniente dallo studio WDR di Colonia è di gran lunga lontana da un'idea di elettronica di Cage); ciò che accomuna entrambe le opere è tuttavia l'esigenza di *animare*[@discipio2021thinking] la musica elettronica attraverso la performance dal vivo.

### Eccezioni tardive e transizione

Verso la fine degli anni '50 iniziarono ad apparire alcune eccezioni a questo dominio esclusivo della tape music da studio [@mumma1975, 292]. Alcuni compositori sperimentarono con suoni registrati su nastro presentati in concerto dal vivo insieme a strumenti o voci, mentre altri collocarono il nastro in situazioni performative innovative che si allontanavano dai riferimenti alla musica tradizionale. Emersero anche tecniche di studio che permettevano elaborazioni instantanee configurandosi di fatto come performance dal vivo, e casi in cui il nastro veniva impiegato esclusivamente per registrare e distribuire i risultati di performance che non lo utilizzavano come medium compositivo primario [@mumma1975, 292].

Il cambio decisivo avvenne quando alcuni compositori — Cage in primis — abbandonarono il nastro come premessa musicale ed esplorarono l'uso di dispositivi elettronici, da soli o in combinazione con strumenti acustici, come componenti per la performance dal vivo [@mumma1975, 292]. Secondo Mumma, questa transizione — dal nastro come medium compositivo primario all'elettronica come strumento performativo — segnò la nascita del live electronics come prassi distinta.  

Durante gli anni '60, la performance dal vivo con suoni amplificati di piccola entità, supportata dallo sviluppo di nuova strumentazione elettronica progettata specificamente per l'uso in concerto, divenne un'attività sempre più rilevante [@mumma1975, 297]. Questa prassi emergente attrasse gradualmente l'attenzione anche di coloro che, inizialmente impegnati filosoficamente con il medium del nastro magnetico, avevano in precedenza considerato il live electronics come un'impresa di scarso interesse [@mumma1975, 297].

## Due pionieri in un panorama globale

Tra il 1960 e il 1965, la scena della musica live electronics conobbe una significativa espansione a livello internazionale. Secondo le ricostruzioni storiche, in questo periodo l'attività si concentrò principalmente negli Stati Uniti, favorita da un vivace clima sperimentale e dalla precoce accessibilità della tecnologia a stato solido [@mumma1975, p. 296]. Accanto a figure americane come Robert Ashley, Alvin Lucier, Gordon Mumma e David Tudor, fiorivano esperienze parallele in Giappone con Takahisa Kosugi, in Francia con Gil Wolman, in Italia con Giuseppe Chiari e in Germania con Karlheinz Stockhausen. Fu in questo contesto di fermento globale che due composizioni, sebbene radicalmente diverse per filosofia e metodo, divennero emblematiche della nuova direzione performativa dell'elettronica: *Cartridge Music* (1960) di John Cage e *Mikrophonie I* (1964) di Karlheinz Stockhausen.

### Un incontro a Colonia: la sfida di Cage

Nell'ottobre del 1960, la prima esecuzione ufficiale di *Cartridge Music* di John Cage ebbe luogo a Colonia, nello studio dell'artista Mary Bauermeister. Secondo le ricostruzioni più attendibili, tra il pubblico era presente Karlheinz Stockhausen [@discipio2021, Parte Terza, p. 314]. Quel concerto rappresentò un momento di potente discontinuità per il panorama elettroacustico europeo. Cage, figlio di un inventore e privo di un solido supporto istituzionale, presentava una musica elettronica costruita non con i costosi strumenti degli studi specializzati, ma con cartucce di giradischi, microfoni a contatto e oggetti quotidiani come stuzzicadenti, molle e piume [@collins2020, p. 40; @chadabe1997, p. 81-82]. L'opera rivelava un intero universo di microsuoni, reso udibile attraverso una semplice amplificazione, e la sua partitura grafica, composta da fogli trasparenti sovrapponibili, istituiva un campo di possibilità aperto all'esplorazione dei performer piuttosto che un oggetto musicale rigidamente definito [@chadabe1997, p. 82]. In questo contesto, Cage coniò per la prima volta l'espressione "live electronic music" [@discipio2021, Parte Terza, p. 314], suggellando la rottura con il paradigma della musica su nastro.

Quattro anni dopo, Stockhausen diede la sua risposta a quella stessa intuizione con *Mikrophonie I*. Se Cage aveva lanciato la sfida della "microfonia" – l'esplorazione attiva del micromondo sonoro attraverso il microfono – Stockhausen ne colse il potenziale per reindirizzarlo all'interno di una poetica del controllo assoluto. L'opera impiega sì un principio simile di amplificazione di suoni minuti, ma lo incanala in un apparato esecutivo di straordinaria complessità e precisione. Sei performer, ciascuno con un ruolo rigidamente definito, agiscono su un tam-tam di grandi dimensioni: due lo eccitano con una gamma di oggetti, due ne esplorano la superficie con microfoni direzionali seguendo scrupolose indicazioni spaziali, e due manipolano filtri e potenziometri per trasformare il suono in tempo reale [@chadabe1997, p. 83-84; @mooney2017]. L'equipaggiamento non è più quello della tecnologia quotidiana, ma è altamente specializzato, incluso un raro filtro Maihak e un tam-tam Paiste appositamente selezionato [@roth2023, p. 61-66]. La partitura, basata sul principio della "moment-form", prescrive nel dettaglio ogni parametro, dalle distanze microfoniche alle frequenze dei filtri, trasformando l'intuizione esplorativa di Cage in un architettura sonora deterministicamente controllata [@chadabe1997, p. 83-84; @roth2023, p. 64].

### Una ricerca comune, due visioni del mondo

Entrambe le opere condividono la scoperta del microfono come strumento attivo, capace di rivelare un "nuovo mondo di risorse sonore" [@mumma1975, p. 296]. La ricchezza dei microsuoni amplificati in *Cartridge Music* era tale da rivaleggiare, come notato da Nicolas Collins, con le sonorità più laboriose prodotte negli studi elettronici europei [@collins2020, p. 40]. Tuttavia, le filosofie sottostanti non avrebbero potuto essere più distanti. Come sintetizza efficacemente Joel Chadabe, la musica di Cage proponeva un'"anarchia felice" basata su processi e giustapposizioni casuali, mentre quella di Stockhausen comunicava "controllo, tecnica ed expertise" [@chadabe1997, p. 83]. Questa divergenza è palpabile nelle loro dichiarazioni: Cage affermava che la sua partitura poteva servire a "esaminare musicalmente una vecchia Volkswagen" [@roth2023, p. 64], mentre Stockhausen sosteneva con forza che sostituire i filtri originali con simulazioni avrebbe distrutto l'unicità storica dell'opera [@roth2023, p. 64].

Questa divergenza di approccio si riflette persino nella terminologia scelta da ciascun compositore per definire la propria musica. Cage, già nel 1962, aveva coniato l'espressione 'live electronic music', ponendo l'accento sull'esperienza performativa ('live') come qualificante principale della musica ('electronic music'). Stockhausen, presentando Mikrophonie I nel 1972, la definì invece 'electronic live music', descrivendo così un genere musicale elettronico ('electronic music') che semplicemente avviene in diretta ('live'). Questa sottile ma significativa inversione lessicale cristallizza le due visioni: da un lato la performance come condizione esistenziale e filosofica, dall'altro il 'dal vivo' come modalità esecutiva di un'opera strutturata.

Sebbene non si possa stabilire un nesso causale certo, il potenziale incontro del 1960 a Colonia cristallizza un passaggio cruciale nella storia del live electronics. *Cartridge Music* funse da potente agente destabilizzante, introducendo nel cuore dell'Europa colta un'idea di elettronica povera, processuale e filosofica. *Mikrophonie I* rappresentò la risposta di un compositore istituzionale, che assorbì il principio della "microfonia" ma lo tradusse nel linguaggio del rigore, della complessità strutturale e del controllo tecnologico specialistico. Come osserva Agostino Di Scipio, erano entrambi "tempi di microfonia, anche se di tipo molto diverso" [@discipio2021, Parte Terza, p. 314]. Da questa comune intuizione nacquero così due traiettorie fondative, destinate a influenzare in modo duraturo lo sviluppo della musica elettroacustica dal vivo.


## Oltre la diade: accenni a un panorama plurale

Sebbene *Cartridge Music* e *Mikrophonie I* dominino la storiografia del live electronics, incarnando rispettivamente l'approccio DIY americano e il modello istituzionale europeo, la loro non fu una rivoluzione solitaria. Tra il 1960 e il 1970, ricerche parallele fiorivano in contesti geografici e culturali profondamente diversi, ciascuna elaborando soluzioni originali al problema di liberare la musica elettronica dal nastro magnetico e riportarla alla dimensione performativa del concerto.

In Italia, Luigi Nono trovò all'ExperimentalStudio der Heinrich-Strobel-Stiftung des Südwestfunks di Friburgo un modello alternativo sia rispetto agli studi radiofonici italiani che alle grandi istituzioni come IRCAM. Dal 1980, come ospite permanente dello studio — e dal 1983 come "artistic advisor" — Nono sviluppò un approccio basato sulla residenza artistica prolungata, trascorrendo ore innumerevoli in studio insieme a interpreti e tecnici in un processo di sperimentazione continua. Come avrebbe ricordato Hans Peter Haller, le parole ricorrenti di Nono erano "unsteady – emotional – searching – always different – always better". Questo "ecosistema di Friburgo" produsse, in meno di un decennio, una serie impressionante di capolavori che spaziavano dal colossale *Prometeo – Tragedia dell'Ascolto* alle più intime *Risonanze erranti*, opere in cui tecniche come i delay multipli e asimmetrici, i pitch shift microtonali e la dislocazione del suono nello spazio divennero la "firma" del compositore. Il modello Nono rappresentava una terza via: né il grande centro di ricerca centralizzato né l'autocostruzione DIY, ma piuttosto un'infrastruttura esistente disposta a investire tempo e risorse per permettere al compositore di sviluppare lentamente una forte visione estetica attraverso la sperimentazione profonda.

Negli Stati Uniti, intorno a figure come Gordon Mumma e Robert Ashley nella città di Ann Arbor, si stava formando un intero ecosistema basato sull'autocostruzione, la collaborazione orizzontale e la portabilità delle tecnologie. Nel 1958, Ashley e Mumma fondarono il Cooperative Studio for Electronic Music, uno studio domestico low-cost esplicitamente concepito per essere accessibile senza affiliazioni istituzionali o ingenti risorse economiche. Il Cooperative Studio forniva musica per filmmaker indipendenti, progettava "cybersonic equipment" per performance dal vivo, e incarnava un principio fondamentale: sacrificando la calibrazione precisa delle grandi attrezzature di laboratorio a favore delle dimensioni compatte dei componenti hi-fi ad alta impedenza, era possibile configurare un sistema dove "l'intero studio fosse alla portata del compositore comodamente seduto" [@mumma2015, p. 19-20].

Questo spirito cooperativo trovò espressione istituzionale nell'ONCE Festival (1961-1965), che presentò complessivamente ventinove concerti con sessantasette prime esecuzioni di circa centocinquanta opere di ottantotto compositori contemporanei [@mumma2015, p. 30]. Significativamente, quando alcuni membri del gruppo tentarono di ottenere supporto dalla University of Michigan, incontrarono "resistenza e persino animosità", con un boicottaggio quasi unanime dei concerti da parte dei docenti della School of Music durante il festival del 1963 [@mumma2015, p. 24, 29-30]. Il supporto venne invece dal Dramatic Arts Center, un'organizzazione locale non universitaria che manteneva un rapporto diretto e immediato con la comunità, senza overhead amministrativo [@mumma2015, p. 30]. L'ONCE Festival non era isolato, ma parte di un ricco ecosistema culturale che includeva lo Space Theatre di Milton Cohen (le cui produzioni culminarono alla Biennale di Venezia nel 1964), l'Ann Arbor Film Festival, e il Sonic Arts Union — quest'ultimo formato nel 1966 quando David Behrman e Alvin Lucier incontrarono Ashley e Mumma durante l'ONCE Festival del 1964 [@chadabe1997, p. 102].

Anche l'Italia vide fiorire scene parallele. A Roma, nel 1964, si formò il Gruppo di Improvvisazione Nuova Consonanza, una collaborazione internazionale che includeva i compositori americani Larry Austin, John Eaton e William Smith insieme agli europei Mario Bertoncini, Aldo Clementi, Franco Evangelisti, Roland Kayn e Ivan Vandor [@mumma1975, p. 316]. Nel 1966, sempre a Roma, venne organizzato MEV (Musica Elettronica Viva), ensemble principalmente di musicisti americani — tra cui Frederic Rzewski, Alvin Curran e Richard Teitelbaum — che nel giro di due anni passò dalla musica composta all'improvvisazione libera fino ad abbandonare nel 1968 "interamente le strutture musicali e sociali formali" [@mumma1975, p. 316]. A Firenze, Giuseppe Chiari compose dal 1964 al 1966 una serie di "azioni musicali" che facevano "largo impiego di microfoni a contatto" [@bernardini1986, p. 68].

Il Giappone sviluppò una scena particolarmente significativa. Takahisa Kosugi compose *Micro 1* (1961), opera per solo microfono, seguita da una serie di lavori poetici con elettronica radio-frequency e audio-frequency [@mumma1975, p. 299]. Toshi Ichiyanagi, che aveva studiato a New York con John Cage tra il 1956 e il 1961, al suo ritorno in Giappone compose un repertorio di opere per strumenti occidentali e giapponesi modificati elettronicamente, tra cui *Space* (1966), *Activities for Orchestra* (1967) e *Music for Living Spaces* (1970), quest'ultima un'installazione per l'Expo di Osaka [@mumma1975, p. 299]. Allo Studio NHK di Tokyo, Joji Yuasa utilizzò uno speciale magnetofono a cinque piste per opere come *Icon (on the source of white noise)* (1967), con diffusione del suono variabile da cinque a venticinque canali indipendenti [@discipio2021, Parte Seconda, p. 273]. La convergenza tra sperimentazione americana e giapponese trovò espressione simbolica nel Cross Talk Intermedia Festival, presentato a Tokyo nel febbraio 1969 nella struttura olimpica di Kenzo Tange progettata da Kenzo Tange, che vide la partecipazione di Mumma, Ashley, Lucier, Reynolds e VanDerBeek insieme a Ichiyanagi, Kosugi, Takemitsu e Yuasa [@mumma1975, p. 318].

Nel Regno Unito, la scena del live electronics si sviluppò con caratteristiche distintive rispetto alle controparti americana ed europea continentale. Il gruppo AMM, formatosi a Londra nel 1965 con Lou Gare, Keith Rowe ed Edwin Prévost (poi raggiunto da Cornelius Cardew nel 1966), incarnò un approccio radicalmente improvvisativo che, pur condividendo con MEV l'uso di elettronica dal vivo, poneva l'accento su preoccupazioni "più sociali che tecnologiche" [@appleton1975, p. 317]. La prassi di AMM consisteva principalmente nell'uso di microfoni a contatto su oggetti quotidiani e strumenti elettromeccanici autocostruiti realizzati riciclando materiale elettronico di scarto [@prevost1995]. Nonostante il retroterra jazzistico dei fondatori, il gruppo cercò di evitare stilemi idiomatici specifici, puntando sull'improvvisazione come strategia per esplorare sonorità trascurate e marginali [@discipio2021, p. 329] – un approccio che Cardew avrebbe teorizzato nel suo influente saggio *Towards an Ethic of Improvisation*, pubblicato all'interno del *Treatise Handbook* [@cardew1971].

Parallelamente, Hugh Davies, dopo aver lavorato come assistente di Stockhausen a Colonia (1964-66), sviluppò a Londra una prassi di costruzione di strumenti elettroacustici da materiali di recupero [@mooney2017]. I suoi *Shozyg* (1968) e *Springboard* (1970-74) – assemblati da molle, lame da seghetto, affettauova – incarnavano quella che Davies definì una filosofia dei "sintetizzatori di musique concrète": strumenti capaci di generare istantaneamente sonorità che negli studi su nastro avrebbero richiesto ore di lavoro [@mooney2017]. Davies propugnava "un *bricolage* elettroacustico molto competente, secondo un approccio di auto-costruzione e auto-gestione dei mezzi che contrastava nettamente con la standardizzazione dei dispositivi di produzione industriale" [@discipio2021, p. 329]. La sua partecipazione a gruppi come *Gentle Fire* e *Music Improvisation Company* contribuì a formare un ecosistema britannico caratterizzato da un DIY working-class e da una forte dimensione improvvisativa, distinguendosi così dalle esperienze sperimentaliste sia del modello istituzionale europeo continentale che dalle formazioni cooperative americane [@Born1995, pp. 59-60].

Questo ricco e policentrico contesto rivela come il live electronics non fosse semplicemente l'antitesi della tape music, ma rappresentasse piuttosto un campo di sperimentazione multiplo in cui convergevano preoccupazioni estetiche, necessità tecnologiche, modelli organizzativi e visioni culturali profondamente diverse. I compositori attivi in questi vari contesti condividevano l'urgenza di superare la fissazione su nastro e recuperare la dimensione performativa, ma elaboravano soluzioni radicalmente diverse: dalla residenza artistica prolungata di Nono a Friburgo, agli studi cooperativi e ai festival indipendenti di Ann Arbor, dai collettivi improvvisativi romani alle collaborazioni transpacifiche. È a questo ecosistema nordamericano — caratterizzato dall'autocostruzione dei circuiti, dalla collaborazione orizzontale, dalla portabilità delle tecnologie e dalla necessità trasformata in filosofia — che dedicheremo il prossimo capitolo, esplorando come figure come Gordon Mumma, David Tudor, Alvin Lucier e i membri del Sonic Arts Union abbiano elaborato non solo nuove tecnologie, ma un modello completamente alternativo di prassi compositiva e performativa.

# Circuiti come partiture: tecnologia DIY e poetiche compositive (1960-1976)
## Contesto tecnologico-economico: dalla necessità alla scelta estetica

### Il transistor come rivoluzione tecnica

La nascita del live electronics americano negli anni Sessanta fu resa possibile da una rivoluzione tecnologica fondamentale: l'invenzione del transistor nel 1947 ai Bell Laboratories e la sua successiva commercializzazione. Come spiega Agostino Di Scipio, fu solo a metà del secolo che divennero disponibili materiali semiconduttori adatti a creare componenti circuitali compatti e versatili. L'elettronica precedente, basata sulle valvole termoioniche, non era riuscita a produrre sistemi musicali sufficientemente stabili ed efficienti, utili in contesti generici e non legati all'esperienza del singolo inventore. La nuova elettronica a transistor, sfruttando il rapporto esponenziale tra tensione in ingresso e corrente in uscita, si presentava invece come la soluzione a quei problemi, aprendo la porta a "possibilità applicative prima impensabili"[@discipio2021, 279].

Questa transizione tecnologica ebbe conseguenze dirette per i compositori. David Behrman ricorda che verso il 1965, lavorare con circuiti autocostruiti era una necessità, poiché non c'erano *synth* disponibili in vendita nel mercato. Tuttavia, Behrman aggiunge una nota cruciale: "You didn't have to have an engineering degree to build transistorized music circuits"[@collins2020, Foreword]. I transistor non solo miniaturizzarono l'elettronica, ma la democratizzarono, rendendola accessibile.

### Il surplus bellico come materiale compositivo

La Seconda Guerra Mondiale lasciò in eredità un'abbondanza di componenti elettronici militari dismessi, disponibili a prezzi irrisori. Gordon Mumma ricorda come David Tudor passasse molto tempo nei negozi di surplus militare a procurarsi transistor e condensatori insoliti. Assemblandoli, si otteneva un circuito unico—un oscillatore, un modulatore—la cui risposta sonora non era replicabile. Sostituendo anche un solo transistor con un altro, il circuito cambiava, dotando ogni strumento di una personalità sonora specifica e irripetibile[@nakai2021, 173-174].

Questa caratteristica—l'*irriproducibilità* intrinseca dei componenti surplus—non fu vista come un limite, ma divenne un valore estetico. You Nakai sintetizza: "Specific components composed specific instruments, which in turn composed specific sound systems—so it was *bias all the way down*"[@nakai2021, 174]. L'approccio americano si distingueva radicalmente dagli studi europei istituzionali: dove Colonia o l'IRCAM perseguivano la precisione e la riproducibilità attraverso equipaggiamento professionale costoso, la scena americana abbracciava l'imperfezione produttiva dei componenti di scarto.

Il contesto economico era tale che, come ricorda Robert Ashley, non si aveva accesso a nulla, se non all'elettronica di base. Prima della fine degli anni Sessanta, i sintetizzatori commerciali semplicemente non esistevano e il mercato era dominato da rivenditori come Lafayette Radio Electronics, che vendeva kit per hobbisti a prezzi accessibili. È significativo che David Tudor conservasse meticolosamente le ricevute per l'acquisto di kit come l'amplificatore a tre transistor PK-522 e il mixer microfonico PA-292, acquistati per pochi dollari[@nakai2021, 121].

Quando i primi sintetizzatori commerciali, come il Moog e il Buchla, arrivarono sul mercato, molti compositori del Sonic Arts Union li rifiutarono esplicitamente. Gordon Mumma tracciava una distinzione fondamentale tra la "explorer tendency" dei musicisti della Cunningham Dance Company e l'omogeneità rappresentata dalla produzione di massa e dai synth commerciali. In questa visione, "The idea of 'product' was fundamental in that regressive cultural tide"[@chadabe1997, 102].

### John Cage: Dal Prepared Piano ai Microsuoni Amplificati

Se nel capitolo precedente John Cage è emerso come pioniere a livello globale, in dialogo e contrapposizione con Stockhausen, per comprendere la specificità del modello americano è necessario considerarlo come l'ispiratore filosofico di un intero movimento (infatti come vedremo in seguito è poi la figura di David Tudor a catalizzare le proposte di Cage in una nuova prassi compositiva). La sua ricerca, iniziata ben prima del seminale *Cartridge Music* (1960), gettò le basi per un approccio alla tecnologia musicale che fosse accessibile, processuale e radicalmente performativo.

La formazione di Cage fu atipica e profondamente segnata da una familiarità con la tecnologia fin dagli anni '30. Figlio di un inventore, assisteva il padre nelle ricerche per i brevetti, sviluppando "una sufficiente padronanza dei concetti e del vocabolario del campo per comunicare con i professionisti" [@mumma2015, 167]. Questo retroterra spiega la sua disinvoltura nell'approccio alla tecnica. La sua curiosità per i nuovi media si manifestò precocemente: tra il 1932 e il 1933 presentava programmi radiofonici a Los Angeles, e nel 1936 lavorava come apprendista montatore con il filmmaker Oskar Fischinger, sperimentando tecniche di sincronizzazione suono-immagine che anticipavano il lavoro sul nastro magnetico [@mumma2015, 167].

Il percorso di Cage verso il live electronics non fu un fulmine a ciel sereno, ma il culmine di una ricerca ventennale sul suono, la performance e la tecnologia, caratterizzata da una grande coerenza di pensiero.

1.  Percussioni e Prepared Piano (anni '40): Già nel 1939, con *First Construction in Metal*, Cage utilizzava "ceppi dei freni e altro ferro vecchio recuperato dalle discariche" (Collins, 2020, p. 40). Il ricorso al prepared piano nello stesso decennio, oltre a creare sonorità inedite, rispondeva a un'esigenza pratica: "avere una risorsa multi-timbrica senza il duro lavoro di spostare strumenti a percussione" (Mumma, 2015, p. 162). L'idea di trasformare oggetti di uso comune in strumenti musicali era già pienamente presente.

2.  La serie "Imaginary Landscape" (1939-1952): Questa serie rappresenta il vero e proprio laboratorio delle sue future idee. *Imaginary Landscape No. 1* (1939) è storicamente significativa come "il primo brano documentato di musica a presentare il DJ come un performer musicale" (Collins, 2020, p. 40), utilizzando giradischi a velocità variabile. Di Scipio (2021a) nota come l'uso performativo del grammofono fosse un'idea dadaista che Cage mutuò da László Moholy-Nagy. Un passaggio cruciale avvenne nel 1942 con *Imaginary Landscape No. 3*, che segnò "il primo uso di piccoli suoni amplificati" da parte di Cage [@mumma2015, 169]. La serie culmina con *Imaginary Landscape No. 4* (1951) per dodici radio, che, insieme a *Radio Music* (1952) e *Music Walk* (1958), esplorò sistematicamente "il ricevitore radio come strumento per la performance dal vivo" (Mumma, 1975, p. 293).

3.  Il periodo del nastro: *Williams Mix* (1952): Sebbene si tratti di un'opera su nastro, *Williams Mix* è fondamentale per la sua metodologia. Cage, con l'aiuto di Earle Brown e David Tudor, creò una biblioteca di suoni catalogati (città, campagna, suoni elettronici, etc.) e utilizzò l'I Ching per decidere tramite operazioni casuali "che tipo di suono utilizzare, su quali tracce posizionarlo, e le durate di suoni e silenzi" (Chadabe, 1997, p. 56). Questo approccio, volto a definire un "territorio" sonoro (in questo caso, il mondo) piuttosto che un oggetto musicale finito, sarebbe diventato centrale nella sua filosofia (Chadabe, 1997, p. 83).

Tutti questi fili si riuniscono in *Cartridge Music*. Alla fine degli anni '50, Cage coinvolse David Tudor nella ricerca di trasduttori elettronici, componenti che andavano diffondendosi con la tecnologia a transistor [@mumma2015, 170]. 
Come nota Gordon Mumma, questo lavoro, assieme alla Music for Amplified Toy Pianos, fu eseguito frequentemente da Cage e David Tudor, essendo "a considerable stimulus to experimentation in live-electronic music"[@mumma1975, 295]. Nicolas Collins sintetizza che la sorprendente ricchezza di questi "microsuoni" enormemente amplificati rivaleggiava con le sonorità sintetiche e costose degli studi europei e "opened the ears of a generation of sound artists to the splendor of the contact mike"[@collins2020, 40].
La partitura di Cartridge Music si basava su un sistema indeterminato ma strutturato. Essa consisteva in materiali grafici su fogli trasparenti che i performer potevano sovrapporre in combinazioni diverse per definire la struttura di una specifica esecuzione. Questo approccio permetteva a Cage di strutturare diverse situazioni performative, incluse quelle collaborative.
David Tudor affrontò queste partiture grafiche con un rigore quasi scientifico, sviluppando un sistema personale per la loro interpretazione. Come documenta You Nakai, Tudor creò template e righelli su misura e elaborò "nomographs" (tradotto nomografi)—sistemi di misurazione grafica—applicando una polarità semplice/complesso a sei parametri musicali[@nakai2021, varie pagine]. In questo modo, la notazione grafica non eliminava la precisione, ma la spostava dalla prescrizione del compositore all'interpretazione attiva e metodica del performer.

L'eredità di Cage per il modello americano che di lì a poco sarebbe fiorito ad Ann Arbor è quindi triplice: filosofica (L'apertura a tutti i suoni e l'uso di processi indeterminati), pragmatica (L'uso della tecnologia quotidiana e accessibile, in opposizione agli apparati degli studi istituzionali) e sociale (La definizione di un "territorio" performativo che valorizza l'esplorazione collettiva).
Queste idee, concretizzatesi nella prassi di David Tudor, avrebbero trovato nel fertile ecosistema di Ann Arbor il terreno ideale per evolversi in un vero e proprio movimento.

## Circuit Music: lo schema circuitale come partitura

La *circuit music* costituisce un paradigma compositivo fondamentale per il live electronics nordamericano. Come definito da Ezra Teboul, essa è caratterizzata dall'adozione di schemi circuitali e diagrammi come sistema notazionale primario, in luogo della partitura tradizionale [@Teboul2023]. In questa prospettiva, il circuito cessa di essere un mero mezzo esecutivo per divenire esso stesso la partitura, ridefinendo la figura del compositore come progettista di sistemi generativi di comportamenti sonori. Nei paragrafi seguenti, questo principio verrà esaminato attraverso le pratiche di coloro che ne sono stati gli interpreti più significativi.

### David Tudor: il caso paradigmatico

David Tudor incarna in modo esemplare l'etica della circuit music. La sua filosofia si basava su un principio di scoperta e ascolto, sintetizzato nella celebre affermazione: *"I try to find out what's there—not to make it do what I want, but to release what's there. The object should teach you what it wants to hear"* [@collins2004]. Questo ribaltamento della relazione tra compositore e strumento, in cui il materiale guida il processo creativo, divenne cardine della pratica performativa. Il suo metodo empirico valorizzava la scoperta diretta delle proprietà intrinseche dei componenti, al di là delle loro specifiche tecniche [@nakai2021, 55].

Questa filosofia maturò attraverso una radicale transizione: da acclamato interprete pianistico delle avanguardie a pioniere autonomo del live electronics. Il punto di svolta fu la sua realizzazione di *Variations II* di Cage, dove il pianoforte amplificato fu concepito non come strumento acustico con elettronica aggiunta, ma come un *autentico strumento elettronico*, il cui comportamento sonoro, plasmato da microfoni a contatto e circuiti di feedback, diveniva intrinsecamente indeterminato.

L'affermazione di questa nuova identità compositiva si dichiarò formalmente in *Fluorescent Sound* (1964), che You Nakai identifica come *"the first work in which he credited himself as a 'composer'"* [@nakai2021, 210-211]. L'opera, manipolando circuiti di luci fluorescenti, esemplifica la logica della circuit music anche nel suo titolo, dove una categoria di componente diviene, per sineddoche, il nome di un'opera. La piena consacrazione giunse con *Bandoneon!* (1966). Nonostante l'esordio fosse un disastro tecnico a causa di circuiti cablati al contrario, l'episodio confermò Tudor come "the ultimate performer", capace di trasformare la precarietà del sistema in un elemento costitutivo della performance.

### Altri volti della circuit music: Mumma, Lucier, Ashley, Behrman

Oltre a Tudor, altri compositori declinarono il principio della circuit music in direzioni distintive, arricchendo il panorama del live electronics nordamericano.

Gordon Mumma sviluppò l'approccio *cybersonico*, un sistema autoreattivo in cui il processamento elettronico modificava il suono attraverso caratteristiche derivate dal suono stesso, creando complessi loop di feedback. Opere come *Hornpipe* (1967) realizzavano questa idea in forma portatile, con un circuito che rispondeva dinamicamente alle caratteristiche del corno. La sua Cybersonics Company (1965) incarnò lo spirito collaborativo e artigianale della scena, producendo dispositivi personalizzati come lo *Spectrum Transfer* per David Tudor. La collaborazione tra i due culminò in *Mesa* (1966), per bandoneon ed elettronica, presentata al seminale evento *9 Evenings: Theatre & Engineering*, simbolo dell'integrazione tra arte, scienza e hardware.

Alvin Lucier orientò la sua ricerca verso l'esplorazione di fenomeni fisici puri. In *I Am Sitting in a Room* (1969), utilizzò il feedback acustico come materiale compositivo, trasformando progressivamente la voce parlata nelle risonanze dello spazio. In *Music for Solo Performer* (1965), le onde cerebrali amplificate (EEG) attivavano strumenti a percussione, inserendo il performer in un sistema di feedback biologico ed elettroacustico. In entrambi i casi, Lucier agiva come progettista di un sistema in cui agenti non-umani—la risonanza ambientale o l'attività cerebrale—diventavano i veri interpreti [@mumma2015; @collins2004].

Robert Ashley sviluppò invece un *electronic music theater*, spostando l'attenzione dalla produzione musicale in senso stretto alla creazione di "situazioni sonore" teatrali. Opere come *The Wolfman* (1964) e *in memoriam... Crazy Horse* (1964) usavano il feedback vocale estremo come drammaturgia, dove il performer diventava un "operatore di feedback" che interagiva deliberatamente con l'instabilità del sistema elettronico [@chadabe1997, 87].

Infine, David Behrman esplorò sistemi a *indeterminacy incorporata*, dove l'imprevedibilità era iscritta nell'architettura del circuito. In *Runthrough*, l'uso di fotocellule permetteva di generare suoni attraverso le ombre dei performer, creando una composizione la cui forma precisa emergeva da azioni imprevedibili [@collins2020]. Behrman inserì esplicitamente questa pratica nella tradizione americana della costruzione strumentale, tracciando una linea ideale da Harry Partch a Tudor e Mumma, e collocando così l'hacking hardware in una precisa genealogia culturale [@collins2020, Foreword].

Questi approcci diversi—dall'autoreattività cybersonica all'ascolto di fenomeni fisici, dal teatro del feedback ai circuiti semi-autonomi—dimostrano come il paradigma della circuit music abbia offerto un comune terreno operativo per esplorazioni radicalmente differenti, pur nella condivisione di un principio fondamentale: la progettazione di sistemi, non la scrittura di suoni.

### L'organizzazione sociale come risorsa

La risposta alla mancanza di supporto istituzionale e di risorse economiche fu l'invenzione di modelli organizzativi basati sulla cooperazione. Il primo e fondamentale esperimento fu il Cooperative Studio for Electronic Music, fondato da Gordon Mumma ad Ann Arbor nel 1960. Questo studio, il primo indipendente del suo genere negli Stati Uniti, si basava su un'idea esplicitamente politica di condivisione delle risorse. La sua natura si distingueva radicalmente dai modelli europei per la mobilità dell'attrezzatura, resa possibile dai transistor, che permetteva di trasformare qualsiasi spazio in un luogo di performance e ricerca sonora [@mumma2015].

Questo spirito collettivo trovò presto una sua vetrina pubblica e internazionale nel ONCE Festival (1961-1965), che operava deliberatamente ai margini delle istituzioni, utilizzando spazi non convenzionali come magazzini e scantinati in una chiara presa di posizione contro il sistema musicale accademico. Da questa matrice comunitaria emerse, nel 1966, una formazione più strutturata: il Sonic Arts Union, fondato da Mumma, Robert Ashley, David Behrman e Alvin Lucier. Il gruppo univa compositori dalle estetiche molto diverse, accomunati dall'interesse per tutti i fenomeni sonori e da una prassi profondamente collaborativa, in cui eseguivano e adattavano le opere gli uni degli altri [@chadabe1997, 102]. I loro estenuanti tour europei furono fondamentali per esportare l'approccio americano, prima che il collettivo si sciogliesse nel 1976, quando i suoi membri furono assorbiti dal mondo accademico.

La filosofia della condivisione della conoscenza e del *learning-by-doing* trovò la sua più piena espressione nella generazione successiva, con il collettivo Composers Inside Electronics, formatosi nel 1973 attorno a un workshop sull'opera *Rainforest* di David Tudor. Come spiega Nicolas Collins, il metodo era empirico e orizzontale: si imparava smontando e replicando gli strumenti del maestro [@collins2004]. Questo modello pedagogico-collaborativo codificò l'etica dell'hacking hardware come un sapere comune, trasmesso attraverso workshop e reti informali. La conoscenza circolava orizzontalmente, con schemi circuitali condivisi "like samizdat literature", formando un patrimonio comune al di fuori della logica della proprietà intellettuale [@collins2020]. Era, come preferisce definirla Collins, una pratica di *DIT (Do It Together)* più che di semplice DIY, che trasformava la precarietà delle condizioni in una rete duratura di collaborazioni.

Questa infrastruttura sociale cooperativa alimentava una precisa posizione estetica e ideologica di resistenza. Il rifiuto dei sintetizzatori commerciali come il Moog e il Buchla non era solo pratico, ma filosofico. David Behrman vi individuava il rischio di un’estetica del cliché, mentre Gordon Mumma vi leggeva una regressione culturale verso la logica del "prodotto" standardizzato [@chadabe1997, 102; @collins2020].
Questa presa di posizione si formalizzò in un’estetica "post-ottimale", teorizzata da Ezra Teboul come scelta deliberata dell'inefficienza e del riuso, uno sguardo su una tecnologia liberata dalla ragione del profitto [@Teboul2023]. La pratica di impiegare componenti surplus e circuiti autocostruiti si configurava così come un gesto critico, allineandosi a una più ampia *estetica del junk* che, nell’arte americana, trasformava lo scarto in commento sociale. In quest’ottica, la precarietà performativa—esemplificata dal fallimento tecnico di *Bandoneon!*—diveniva il tratto distintivo di un approccio che valorizzava l’esplorazione e l’instabilità sopra l’affidabilità omogenea del commerciale.

### Dalla partitura al network: una ridefinizione dell’atto compositivo

La convergenza di questi principi—circuit music, cooperazione, post-optimalità—ridefinì l’atto compositivo nelle sue fondamenta. Come rifletteva David Tudor, si passò dal comporre musica al *comporre strumenti* che generano musica. Questo spostamento inaugurò una forma di *co-composizione*, sfumando i confini d’autorialità. La celebre massima di Tudor, *"the object should teach you what it wants to hear"* [@collins2004], attribuiva un’agency decisiva al dispositivo: il circuito, con le sue specifiche proprietà e *bias*, diventava un collaboratore attivo.

In definitiva, fu l’organizzazione sociale stessa a emergere come forma compositiva allargata. La rete di conoscenze, strumenti e pratiche condivise costituiva un vero e proprio *commons* tecnologico, un ecosistema alternativo descritto da Nicolas Collins come "part of a community of shared knowledge" anziché proprietà intellettuale [@collins2020, 6]. Questo modello, anticipando di decenni le logiche del *software libero* e della produzione *peer-to-peer*, sanciva il passaggio dalla musica della notazione alla *musica del network*. Il circuito fungeva così simultaneamente da partitura e da protocollo per una cultura collaborativa, fondando l’eredità radicale di questa scena.

## Il modello britannico: improvvisazione e materialità

Come accennato nel capitolo precedente, la scena londinese di fine anni Sessanta sviluppò un approccio al live electronics che,pur condividendo con la tradizione americana la necessità economica dell'autocostruzione e il rifiuto della mediazione istituzionale, articolò queste istanze con specifiche modalità organizzative e priorità estetiche [@discipio2021, p. 329; @born1995RationalizingCulture, pp. 59-60]. Se i compositori del Sonic Arts Union avevano elaborato un modello di cooperativa performativa centrata sulla tecnologia come linguaggio compositivo, figure come Hugh Davies e gruppi come AMM enfatizzarono invece la dimensione radicalmente improvvisativa e la materialità degli oggetti quotidiani come agenti sonori. Entrambi i contesti condivisero la pratica del bricolage elettronico e l'impiego di componenti di scarto, ma mentre gli americani attinsero principalmente al surplus militare, la scena britannica privilegiò materiali domestici, sviluppando una sensibilità working-class che negli anni Settanta si sarebbe esplicitata in termini ecologici e politici. Vale ora approfondire queste convergenze e specificità, illuminando come le condizioni economiche e istituzionali simili producessero soluzioni tecniche affini ma filosofie performative distintive.

### Hugh Davies e la filosofia del materiale di scarto

Dopo due anni come assistente di Karlheinz Stockhausen a Colonia, Hugh Davies si trovò, al suo rientro a Londra nel 1967, in una situazione paradossalmente simile a quella dei colleghi americani. Privato dell'accesso all'attrezzatura sofisticata dello studio tedesco e senza le risorse per procurarsene una propria, Davies trasformò questo limite in un catalizzatore creativo.

La svolta avvenne grazie all'incontro con Annea Lockwood, che gli mostrò il potenziale musicale di oggetti comuni, come dei fischietti per bambole, aprendolo a un universo sonoro "far removed from the avantgarde context I had largely worked in beforehand" [@davies2002soundsheard, 55]. Da qui, tra il 1968 e il 1974, sviluppò la sua pratica costruttiva, dando vita agli strumenti della serie *Shozyg* e *Springboard*. Questi assemblaggi amplificati utilizzavano oggetti di recupero domestico—dalle molle metalliche alle scatole del pane—attivati da microfoni a contatto e pickup ricavati da vecchie cornette telefoniche.

Il documentarista David Roberts notò come Davies adottasse sistematicamente un linguaggio che attribuiva agency allo strumento, usando frasi come "the instrument tells me what to do". Questa filosofia, che vedeva nel materiale un interlocutore attivo, lo avvicinava all'approccio dei compositori americani, pur applicandola a un repertorio di oggetti radicalmente diverso e quotidiano.

Da necessità pratica, la sua ricerca si consolidò in una precisa posizione estetica ed ecologica. Davies motivava il suo lavoro come un atto di sensibilizzazione, dichiarando: "I invent new musical instruments for people to enjoy new experiences and to increase their sensitivity to the environment of our polluted world, as a small gesture against consumerism and the tendency to throw everything away instead of recycling the waste materials of our society" [@davies2002soundsheard, 31]. Questa visione, emersa nel clima di crisi economica e crescente coscienza ambientale degli anni Settanta in Gran Bretagna, si propose esplicitamente come correttivo allo spreco della società dei consumi, dimostrando attraverso workshop come gli scarti potessero avere una seconda vita utile.

Davies stesso definì i suoi strumenti "sintetizzatori di musique concrète", concettualizzando una significativa cortocircuitazione storica. La sua pratica portava infatti la logica fondante della *musique concrète*—l'esplorazione e manipolazione di suoni reali—direttamente nella dimensione della performance istantanea, senza la mediazione dello studio su nastro. Questo approccio democratizzava tecniche compositive un tempo confinate nelle istituzioni, rendendole accessibili attraverso l'artigianato e il riuso del domestico.

### AMM e l'etica dell'improvvisazione collettiva

La prassi improvvisativa di AMM si distinse nettamente dalle formazioni americane per la radicalità del suo approccio collettivo. A differenza del modello del "repertory ensemble", in AMM scomparve ogni distinzione tra compositore e performer, fondendo le identità in un'unica entità creativa. Keith Rowe descrisse questa pratica come una ricerca collettiva di suoni, un processo esplorativo condiviso dove l'identità del gruppo prevaleva costantemente su quelle individuali[@prevost1995]. La strumentazione rifletteva questo rifiuto: la chitarra di Rowe, suonata orizzontalmente con oggetti, le radio a onde corte di Prévost e le percussioni archettate abbandonavano consapevolmente i loro linguaggi tradizionali.

Cornelius Cardew teorizzò questa filosofia nel saggio *Towards an Ethic of Improvisation* (1971), dove tre virtù risultarono particolarmente significative. La *Semplicità* era intesa come distillazione essenziale che "must contain the memory of how hard it was to achieve"[@cardew1971]. L'*Integrità* spostava il focus dall'intenzione all'azione concreta, mentre l'*Accettazione della Morte* riconosceva la transitorietà della musica improvvisata come sua caratteristica fondamentale.

Questa etica trovò radicalizzazione nella *Scratch Orchestra*, che democratizzò il ruolo del performer permettendo a chiunque, indipendentemente dalle competenze, di partecipare[@born1995RationalizingCulture, pp. 59-60]. Il suo "Research Project" era obbligatorio per tutti i membri, senza distinzioni, e i concerti si tenevano in spazi non convenzionali, rifiutando la sacralità della sala da concerto. Cardew concepì l'orchestra come incarnazione di ideali insieme musicali, educativi e sociali.

Questa priorità sociale distinse l'approccio britannico da quello americano. Mentre negli Stati Uniti si manteneva la competenza tecnica come prerequisito, in Inghilterra si eliminò ogni barriera tecnica per un'accessibilità totale. Come osservato da Hugh Davies, le partiture verbali dell'epoca riducevano il controllo tradizionale del compositore a favore di un coinvolgimento più ampio[@davies2002soundsheard, 14]. La tecnologia venne così subordinata all'obiettivo di creare spazi di produzione musicale orizzontali e non gerarchici.

# IRCAM E IL MODELLO ISTITUZIONALE

## Genesi di IRCAM: dalla crisi del serialismo all'istituzione (1970–1977)

### La crisi compositiva del serialismo e la necessità di un centro di ricerca

Verso la fine degli anni '60, il serialismo integrale — quella promessa di rinnovamento totale del linguaggio musicale che aveva dominato l'avanguardia europea del dopoguerra — mostrava segni evidenti di stallo. L'ambizione di rifondare completamente le basi della "lingua" musicale occidentale, di fornire un sistema universale per la composizione paragonabile alla tonalità, era gradualmente implosa sotto il peso delle sue stesse contraddizioni [@born1995, pp. 2-3]. Quella che Boulez stesso aveva concepito come una *certezza* — persino una "certezza sull'incertezza" nei processi aleatori — era ora circondata da un clima di dubbio intensificato sul lascito del modernismo serialista [@born1995, p. 3].

Per Boulez, il problema non era meramente estetico ma profondamente compositivo. Come osservato da diversi commentatori, la sua produzione compositiva declinò drasticamente dopo la metà degli anni '60 [@born1995, p. 84]. Boulez stesso riconobbe che il progetto IRCAM fu concepito per superare certi "ostacoli fondamentali" nel suo lavoro compositivo — ostacoli che, tuttavia, egli considerava caratteristici della composizione contemporanea nel suo insieme [@born1995, p. 353, n. 11]. La soluzione non poteva venire da un semplice ripensamento individuale della tecnica compositiva, ma richiedeva un'infrastruttura che permettesse di esplorare nuove direzioni attraverso la ricerca scientifica e lo sviluppo tecnologico.

### Il modello "music research": scientificizzazione del processo compositivo

La visione di Boulez per IRCAM si radicava in un'idea specifica di "ricerca musicale" (*music research*) che aveva iniziato a prendere forma già negli anni '60 in ambito francese. Questo concetto, che si richiamava alle ricerche condotte dal Groupe de Recherches Musicales (GRM) di Schaeffer intorno alla *musique concrète*, proponeva un approccio sistematico alla composizione basato sull'analisi scientifica dei materiali sonori [@born1995, pp. 85-86]. Il *music research* si configurava come un processo che integrava la ricerca acustica e psicoacustica con lo sviluppo tecnologico, alimentando a sua volta la composizione attraverso la creazione di nuovi materiali sonori e strutture musicali, oltre che di nuove tecnologie.

Ciò che Boulez intendeva realizzare andava tuttavia oltre il modello del GRM. La "ricerca musicale" doveva diventare razionalizzazione del "linguaggio musicale" stesso in termini scientifici: un tentativo di scientificizzare non solo i materiali sonori ma l'intero processo compositivo, trasformandolo da pratica individuale a processo collaborativo e istituzionale [@born1995, pp. 85-86]. Questa visione si inseriva perfettamente nella tendenza della musica serialista del dopoguerra — specialmente nelle figure di Boulez, Stockhausen e Babbitt — verso il ricorso crescente ai media elettronici e alla teoria scientista [@born1995, pp. 2-3]. La musica doveva dialogare con l'acustica, la fisica, la matematica, la statistica, la teoria dell'informazione, la logica, la linguistica: un vero e proprio arsenale di discipline scientifiche chiamate a rifondare la composizione su basi rigorose.

### L'incontro con Pompidou: convergenza di visioni (1970)

L'occasione per tradurre questa visione in realtà istituzionale giunse in un momento politico particolare. Nel 1970, il presidente Georges Pompidou stava progettando il Centre Georges Pompidou (CGP), il nuovo Museo Nazionale d'Arte Contemporanea che avrebbe dovuto incarnare la ricostruzione della cultura francese dopo la crisi del '68. Durante una cena all'Eliseo, Pompidou — insoddisfatto delle relazioni ostili tra Boulez e l'establishment culturale francese e del suo "esilio" quasi decennale all'estero — offrì al compositore una *carte blanche* per progettare il centro di ricerca musicale di cui aveva parlato negli anni recenti [@born1995, pp. 73-74, 83-84]. L'invito era esplicito: prendere parte al rinnovamento della vita artistica francese e, specificamente, progettare l'"ala musicale" del CGP.

L'incontro rappresentò la convergenza di due visioni distinte ma complementari. Pompidou cercava istituzioni centralizzate che fossero al contempo proiettate verso il futuro e capaci di attrarre un pubblico ampio — un'ideologia in cui centralizzazione e arte d'élite si proponevano compatibili con il populismo [@born1995, p. 73]. Boulez, dal canto suo, vedeva nell'istituzionalizzazione della ricerca musicale l'unica via per superare lo stallo compositivo dell'avanguardia serialista. Il risultato fu la decisione di affidare direttamente a Boulez — un'indicazione del ruolo prominente che gli intellettuali francesi hanno potuto rivendicare nella vita pubblica e nelle cariche politiche — il potere di progettare e dirigere ciò che sarebbe diventato IRCAM.

### Pianificazione, costruzione e status giuridico (1970–1977)

Il periodo dal 1970 al 1977 fu dedicato principalmente alla pianificazione, costruzione e sviluppo dell'istituto [@born1995, pp. 103-104]. Nel 1975 alcune ricerche erano già iniziate nel vecchio edificio, e nel 1976 venne fondato l'Ensemble Intercontemporain (EIC), orchestra dedicata alla musica contemporanea e moderna destinata a godere di una relazione speciale con IRCAM. Ma il passo cruciale fu compiuto nel 1977, quando IRCAM ottenne uno status giuridico insolito: da istituzione pubblica — il dipartimento musicale del CGP — divenne un'associazione privata semi-autonoma con propri statuti, mantenendo però legami importanti con il CGP [@born1995, p. 106].

Questa autonomia amministrativa era presentata come riflesso dello status eccezionale e del privilegio di IRCAM in quanto organismo coinvolto nell'*originazione creativa* e nella produzione culturale, in contrapposizione alla maggior parte degli organi culturali statali (incluso lo stesso CGP) confinati alla riproduzione culturale — archivi, esibizioni, collezioni, esecuzioni [@born1995, pp. 106-107]. In quest'opposizione classificatoria tra produzione e riproduzione culturale, la produzione veniva percepita come dotata di status superiore. Praticamente, l'autonomia garantiva a IRCAM flessibilità gestionale e finanziaria, la possibilità di assumere stranieri, e la capacità di ricevere mecenatismo privato — vantaggi negati agli altri centri di ricerca musicale che dipendevano dalla Direction de la Musique.

L'inaugurazione ufficiale del CGP e di IRCAM avvenne nel 1977, celebrata con la monumentale serie di concerti *Passage du Vingtième Siècle*: circa 115 compositori moderni eseguiti in una settantina di concerti distribuiti nell'arco di dodici mesi in diverse sedi di Parigi [@born1995, pp. 103-104, 170-171]. La scala sostenuta di questa serie era del tutto unica nella storia della musica contemporanea: un tentativo di conquistare per la Francia, attraverso questa dichiarazione fondativa, il ruolo chiave e legittimante per la musica futura. Il ritorno di Boulez a Parigi per dirigere IRCAM aveva l'aria del "figliol prodigo" e attirò grande attenzione pubblica [@born1995, pp. 83-84].

### Un modello alternativo: razionalizzazione e centralizzazione

Con IRCAM prendeva forma un modello radicalmente diverso rispetto alle esperienze di live electronics che stavamo fiorendo negli Stati Uniti, in Gran Bretagna e altrove. Mentre compositori come Mumma, Tudor, Lucier e i membri del Sonic Arts Union elaboravano prassi basate sull'autocostruzione di circuiti, sulla collaborazione orizzontale e sulla portabilità delle tecnologie [@mumma1975; @collins2020], IRCAM proponeva una visione opposta: la trasformazione del lavoro creativo della composizione da attività individuale a processo istituzionale, la sua razionalizzazione all'interno di una divisione del lavoro che includeva non solo la composizione ma anche ricerca scientifica e sviluppo tecnologico correlati [@born1995, pp. 85-86].

Era l'incarnazione di una tendenza verso la centralizzazione e la razionalizzazione che, come vedremo, si sarebbe manifestata a molti livelli distinti: istituzionalmente e amministrativamente; nei "contenuti" dell'estetica di IRCAM, focalizzata sul contributo della scienza e della tecnologia alla composizione; nelle pratiche di produzione tecnologica e nell'estesa divisione del lavoro; e nei crescenti tentativi di amministrare persino la "domanda" attraverso marketing e ricerche di mercato [@born1995, p. 29]. 

Se Ann Arbor, Londra e Tokyo stavano sviluppando ecosistemi basati su DIY, cooperazione e necessità trasformata in filosofia, Parigi stava costruendo quello che sarebbe diventato il più grande centro dedicato alla musica contemporanea e computerizzata del mondo — un'istituzione che rappresentava, nell'istituzionalizzazione della creatività stessa, una nuova partenza nell'istituzionalizzazione della musica [@born1995, p. 2].

## Centralizzazione tecnologica e "grandi macchine"

### Il 4X: tecnologia real-time e sintesi digitale

Nel 1975, il compositore Luciano Berio — allora co-direttore di IRCAM — portò all'istituto il fisico Giuseppe Di Giugno con una richiesta apparentemente impossibile: voleva "un sintetizzatore real-time con mille oscillatori" [@chadabe1997, p. 180]. La richiesta nasceva da un'esigenza compositiva precisa: superare i limiti dei nove oscillatori dello Studio di Fonologia di Milano, dove Berio aveva lavorato negli anni '50, e recuperare quella capacità di sintesi in tempo reale che era andata perduta con i primi sistemi di sintesi digitale basati su computer [@born1995, pp. 185-186].

Il progetto si concretizzò attraverso una serie di prototipi progressivi. Il 4A (1976), primo tentativo, permetteva di sommare molte forme d'onda per creare suoni complessi ma i suoi oscillatori software non potevano interagire tra loro [@chadabe1997, p. 181]. Il 4B (1977), sviluppato in collaborazione con Hal Alles, introdusse sessantaquattro oscillatori interconnessi. Il 4C (1979), il primo utilizzato da diversi compositori, spostò l'enfasi verso la programmabilità, abbandonando il concetto di "oscillatori" in favore di "oggetti con algoritmi particolari" che potevano essere collegati tra loro [@chadabe1997, p. 181]. Come spiegò Di Giugno, "i sintetizzatori devono essere fatti per i musicisti, non per le persone che li costruiscono" — una filosofia che lo portò a cercare una maggiore generalità e flessibilità [@chadabe1997, p. 181].

Il 4X definitivo, presentato pubblicamente al Festival di Donaueschingen nel 1981 durante una prima esecuzione di *Répons* di Boulez, rappresentava il culmine di questo processo evolutivo. La macchina era capace sia di sintesi che di analisi del suono in tempo reale e offriva una gamma impressionante di tecniche di elaborazione: sintesi additiva, sintesi sottrattiva mediante filtraggio numerico, sintesi FM, campionamento e processamento di suoni acustici, modulazione ad anello, armonizzazione, eco, riverbero, *phasing*, analisi di frequenza e spettrale [@born1995, pp. 185-186; @chadabe1997, p. 182]. La sua potenza e generalità lo rendevano unico nel 1984, sebbene la sua posizione fosse in qualche modo minacciata da un rivale commerciale più potente in arrivo: il Lucasfilm ASP [@born1995, p. 186].

L'intenzione progettuale della macchina era fornire un'architettura hardware di base per l'elaborazione del segnale che potesse essere utilizzata in modo flessibile per diverse tecniche di sintesi. Tuttavia, una delle principali debolezze del progetto risiedeva nel fatto che l'hardware da solo non era sufficiente per fornire questa flessibilità: era necessario lo sviluppo di software appropriato e periferiche per rendere la macchina pienamente e musicalmente utilizzabile, e questa fase cruciale venne trascurata nei primi anni del progetto [@born1995, pp. 185-186]. BU, il designer, non era interessato al software e aveva scarsa considerazione per quelli che vedeva come le pretese degli "intellettuali del software" di IRCAM [@born1995, p. 186]. Così nel 1984, tre anni dopo l'emergere dell'hardware finale, il lavoro principale legato al 4X riguardava lo sviluppo di software e periferiche come DAC, ADC e vari dispositivi di controllo gestuale, tutti finalizzati alla realizzazione di una "4X workstation musicale" [@born1995, pp. 186-187].

I compositori che utilizzarono il 4X con maggior successo furono quelli residenti a IRCAM, come Boulez stesso, alcuni tutors e junior tutors, e il direttore della Music Research [@born1995, p. 186]. Il Music Research director lodò altamente la flessibilità e la risposta istantanea del 4X, che sentiva consentisse un lavoro empirico e la possibilità di provare idee compositive in un ambiente reattivo [@born1995, p. 186]. Per lui, era il 4X piuttosto che il software avanzato di IRCAM a incarnare l'esperimento più utopico: favoriva metodi compositivi in tempo reale e, con il completamento della ricerca in corso, sarebbe stato suscettibile di controllo gestuale e facilmente adattabile per usi in performance dal vivo — una tecnologia che poteva contrastare la tendenza high-tech a spostare la performance dal vivo [@born1995, p. 186].

### Esclusività tecnologica e contraddizioni produttive

Tuttavia, la storia del 4X rivela dinamiche istituzionali che andavano ben oltre le questioni puramente musicali. Verso la fine del 1983 e l'inizio del 1984, si svolsero negoziati prolungati con un'azienda chiamata Sogitec, che produceva componenti per aerei ed era strettamente legata all'industria della difesa [@born1995, pp. 109-110]. Significativamente, Sogitec non era interessata alle capacità musicali del 4X: il direttore dell'industrializzazione vendette la macchina trovando un modo di usarla per simulare il rumore degli aerei, e l'azienda la acquistò per diventare la base di un simulatore di rumore di volo [@born1995, p. 110]. Nel luglio 1984, Sogitec fu improvvisamente rilevata dal gigante della difesa Dassault — produttore di aerei e armi high-tech — il che ritardò ulteriormente i tempi di produzione [@born1995, p. 110].

Alla fine, IRCAM diede a Sogitec il prototipo del 4X come base per la produzione in cambio di sole quattro unità 4X e una piccola royalty [@born1995, p. 110]. Il 4X fu quindi acquisito da un'azienda francese di primo piano e valorizzato come industrialmente utile. Tuttavia, poiché Dassault/Sogitec non aveva interesse per il mercato musicale, il 4X non fu mai prodotto come sintetizzatore musicale commerciale, né ne furono prodotti abbastanza da distribuire ad altri centri di computer music [@born1995, p. 110]. Il costo della macchina era approssimativamente $100,000 [@chadabe1997, p. 183] — una cifra che ne limitava drasticamente l'accessibilità.

La saga 4X-Sogitec fu tenuta sotto silenzio durante il 1984 e non se ne parlava liberamente all'interno di IRCAM [@born1995, p. 110]. Pochissimi lavoratori menzionarono confidenzialmente di essere turbati dalle implicazioni militariste del deal, e ugualmente dal fallimento del 4X nel raggiungere un pubblico musicale più ampio, ma la maggior parte rimase in silenzio [@born1995, p. 110]. Il designer del 4X, BU, era arrabbiato, e si diceva che avesse rifiutato di dare a Sogitec qualsiasi progetto scritto per la macchina, così dovettero elaborare le sue operazioni da zero — un fattore nei lunghi ritardi che significarono che le unità 4X arrivarono in ritardo da Sogitec, scomodamente vicino alle prime di *Répons* [@born1995, p. 110].

Questo rifiuto di documentare si collegava a un mito fondativo di IRCAM. Berio, che aveva portato BU a IRCAM e gli aveva chiesto di costruire quello che sarebbe diventato il 4X, aveva decretato che IRCAM dovesse essere una "cultura orale", che trasmettesse informazioni da persona a persona, centrata sul suono [@born1995, p. 235]. Paradossalmente, quello che era iniziato come un principio anarchico-utopico divenne, intorno al 4X, una preservazione della segretezza che incoraggiava un feticismo del mistero della macchina [@born1995, p. 235].

IRCAM abitava quindi in questo periodo una contraddizione. Per statuto, mirava a sviluppare ricerca e tecnologie innovative del tipo che il settore privato non può o non vuole produrre. Eppure paradossalmente, le tecnologie erano poi di scarso interesse per il settore commerciale, così rimanevano quasi una tecnologia artigianale — pochissime furono mai prodotte [@born1995, pp. 110-111]. Avevano quindi una circolazione minuscola, poca influenza più ampia e scarso valore economico. Dato il quadro ideologico dominante e quello legale che circondava IRCAM, la probabilità che guadagnasse dal suo hardware prototipale era piccola. Certamente guadagnò poco dal 4X [@born1995, p. 111].

Ma questa mancanza di validazione commerciale era compensata dal fatto che la tecnologia manteneva il massimo valore simbolico sulla scena culturale proprio grazie alla sua unicità [@born1995, p. 110]. Evitando la commercializzazione, il 4X non veniva svalutato dall'entrare in un grande mercato. Rimaneva uno "strumento" esclusivo e prestigioso che poteva essere utilizzato solo a IRCAM. Aggiungeva quindi massimo prestigio ai pochi pezzi come *Répons* che, realizzati a IRCAM, avevano un monopolio virtuale sul suo uso [@born1995, p. 110]. Senza tale esclusività, IRCAM avrebbe potuto sembrare avere poche strutture uniche da offrire ai compositori.

### Mediazioni tecnologiche e complessità del software

Ma il 4X rappresentava solo una parte — seppur centrale — dell'architettura tecnologica di IRCAM. Parallelamente allo sviluppo hardware, l'istituto stava elaborando un complesso ecosistema software che introduceva mediazioni di natura radicalmente diversa rispetto al live electronics DIY americano. Mentre compositori come Mumma e Tudor lavoravano con la manipolazione diretta di circuiti hardware, dove il rapporto tra gesto e suono era immediato e tangibile, IRCAM sviluppava sistemi basati su una gerarchia verticale di codici software che interponeva multiple traduzioni tra l'intenzione compositiva e il risultato sonoro [@born1995, pp. 226-232].

Questa "multitestualità" intrinseca del software costituiva una caratteristica chiave del medium [@born1995, p. 226]. L'uso e lo sviluppo del software comportavano la scrittura di istruzioni codificate all'interno di un linguaggio software, o la creazione di un linguaggio completamente nuovo, nel contesto di una gerarchia di tali linguaggi. A ogni livello della gerarchia, avveniva una traduzione tra due linguaggi o livelli di codice adiacenti. Le istruzioni del linguaggio di livello superiore dovevano essere tradotte in una forma che potesse essere "letta" ed eseguita dal codice o linguaggio di livello inferiore senza alcuna (o con minima) perdita di "significato" [@born1995, p. 226].

La gerarchia di codici che normalmente operava nel software del computer includeva, al livello più basso, il codice macchina — le istruzioni che guidano l'hardware, scritte in forma binaria; al livello successivo, il codice assembler, costituito da abbreviazioni mnemoniche del codice macchina; sopra questo, il sistema operativo generale (come UNIX) che forniva un framework di base e un insieme di servizi; e sopra questo, uno qualsiasi dei principali linguaggi come FORTRAN, Pascal, C o LISP [@born1995, pp. 226-227]. Il punto essenziale di questi linguaggi di livello superiore era che fornivano modi condensati di esprimere molte migliaia di operazioni di livello inferiore in assembler o codice macchina; quindi istruzioni estremamente complesse potevano essere codificate con economia. La logica era anche che, rispetto al linguaggio assembly, fornissero forme di espressione concettualmente più significative [@born1995, p. 227].

Il software di computer music come quello utilizzato e prodotto da IRCAM aggiungeva però un ulteriore livello di mediazione, gerarchia e traduzione, poiché i linguaggi musicali erano essi stessi basati su, o scritti in, linguaggi generali consolidati [@born1995, pp. 227-228]. Così, Music V era scritto in FORTRAN, Cmusic in C, Chant di IRCAM in FORTRAN, e Formes in LISP. Questa stratificazione aveva implicazioni profonde per la pratica compositiva: ogni layer costituiva una distanza ulteriore dal controllo sonoro diretto, trasformando la composizione in una forma di programmazione multi-livello che contrastava radicalmente con l'empirismo del tape music analogico o del live electronics DIY [@born1995, pp. 226-228].

La densità delle mediazioni tecnologiche e codificate era quindi molto maggiore a IRCAM rispetto alle musiche tradizionali basate su partitura o alle tecniche empiriche della musica elettronica analogica [@born1995, p. 231]. La figura 9 nel libro di Born mostra anche le diverse relazioni degli utenti "naive" e "skilled" con le mediazioni verticali della tecnologia [@born1995, p. 227]. Per gli utenti non esperti, era impossibile intuire la logica implicita dei codici, quindi il loro uso richiedeva guida e applicazione prolungata. Anche allora, a differenza dei programmatori esperti, gli utenti naive imparavano a controllare e interagire solo con il livello superficiale della gerarchia dei codici [@born1995, pp. 230-231]. Quindi, se c'era un bug, erano impotenti a entrare nei livelli inferiori della gerarchia per capire e correggere cosa non andasse, proprio come erano relativamente impotenti a modificare o migliorare il sistema nel complesso come avrebbero potuto desiderare [@born1995, p. 231].

### Chant/Formes: AI e sintesi vocale come avanguardia alternativa

Parallelamente al progetto 4X, IRCAM sviluppava un'altra linea di ricerca che si proponeva come alternativa teorica e tecnologica: il gruppo Chant/Formes, che lavorava su software musicale avanzato ispirato dall'intelligenza artificiale [@born1995, pp. 187-189]. Chant era un sistema di synthesis-by-analysis focalizzato sulla voce, mentre Formes offriva strumenti compositivi basati su AI. Entrambi giravano sul VAX — quindi non in tempo reale — ed enfatizzavano il controllo strutturale musicale di alto livello piuttosto che la potenza di elaborazione hardware [@born1995, pp. 187-188].

Il gruppo Chant/Formes si auto-definiva come "avanguardia scientifica" di IRCAM, rivendicando collegamenti con l'AI come "leading edge" della ricerca [@born1995, pp. 188-189]. Questa auto-rappresentazione produceva una delle tensioni più significative all'interno dell'istituto: mentre il 4X incarnava l'ideologia della "grande macchina" hardware e della potenza di calcolo bruta, Chant/Formes proponeva una filosofia utopica articolata intorno all'idea che il software high-level potesse democratizzare l'accesso alla tecnologia, permettendo ai compositori di creare ambienti software personalizzati per sé stessi [@born1995, pp. 188-189, 230-231].

I pochi critici interni del 4X — tra cui il gruppo Chant/Formes era il più vocale — lo descrivevano come troppo generalizzato, semplicemente una simulazione grandiosa e primitiva di tecniche analogiche: "un glorificato patchboard con mille oscillatori" [@born1995, p. 187]. Sostenevano che la tecnologia del 4X fosse ormai datata, che mancasse di controlli musicalmente appropriati e trascurasse il potenziale del computer per lo sviluppo musicale-concettuale di livello superiore — preoccupazioni che erano la base del loro stesso lavoro software [@born1995, p. 187].

Era però proprio questo problema — la capacità di permettere agli utenti di creare ambienti software personalizzati — che non poteva sfuggire al carattere materiale del software: anche con un programma interattivo (come Chant), c'erano ancora livelli di codice sottostanti il programma che gli utenti naive non avevano le competenze per entrare e modificare [@born1995, p. 231]. Il miraggio dell'AI come democratizzazione dell'accesso tecnologico si scontrava con la realtà delle mediazioni verticali: l'opacità del sistema rimaneva, semplicemente spostata a un livello diverso.

La tensione tra 4X e Chant/Formes — tra hardware real-time e software high-level, tra potenza bruta e sofisticazione concettuale — rivelava una frattura fondamentale nelle visioni tecnologiche di IRCAM. Ma entrambi i progetti condividevano una caratteristica comune: la centralizzazione della conoscenza in poche mani esperte, la creazione di tecnologie esclusive che richiedevano mediazione istituzionale per essere utilizzate, e la trasformazione della composizione da pratica individuale diretta a processo collaborativo altamente mediato. Era precisamente in questo che il modello IRCAM si distanziava radicalmente dalle pratiche DIY del Sonic Arts Union e dei loro colleghi britannici — un contrasto che il prossimo capitolo esplorerà in dettaglio.


## Répons (1981): case study del metodo IRCAM

### Concezione dell'opera e sviluppo simbiotico con il 4X

Répons fu concepito fin dall'inizio non semplicemente come opera musicale, ma come dimostrazione delle capacità tecnologiche di IRCAM. Come osserva Bernardini, l'opera "era destinata a mostrare sia la virtuosità dell'Ensemble Intercontemporain che lo stato dell'arte tecnologico di IRCAM" [@bernardini1986]. La genesi dell'opera fu intimamente legata allo sviluppo del 4X stesso: storicamente, il progetto del 4X fu "strettamente coinvolto" (closely implicated) proprio con Répons di Boulez [@born1995, p. 140]. Questo sviluppo "simbiotico" significava che le esigenze compositive di Boulez guidavano direttamente l'evoluzione tecnologica della macchina, mentre le possibilità emergenti del 4X aprivano nuove direzioni compositive.
L'organico dell'opera prevedeva ventiquattro musicisti e sei solisti — pianoforte, sintetizzatore, arpa, cimbalom, vibrafono e xilofono — i cui suoni venivano trasformati indipendentemente e diffusi attraverso diversi altoparlanti disposti intorno alla sala [@chadabe1997, p. 182]. La struttura drammaturgica era concepita teatralmente: dopo una parte introduttiva strumentale brillante, l'ingresso teatrale dei solisti portava con sé una serie di trasformazioni live electronics che generavano echi trasformati delle parti dei solisti nello spazio [@bernardini1986]. Non si trattava quindi semplicemente di amplificazione o elaborazione del suono, ma di una vera e propria moltiplicazione spaziale e timbrica dei gesti solistici, dove il 4X agiva come mediatore tra l'azione strumentale e la sua proiezione trasformata nell'ambiente acustico.

### Division of labor: il ruolo del tutor e la questione dell'authorship

Tuttavia, la realizzazione di Répons non fu opera di Boulez da solo, ma il risultato di una collaborazione prolungata con il suo tutor non ufficiale, BYV, che divenne direttore del 4X Software [@born1995, pp. 264-268]. Questa collaborazione rivela in modo esemplare il modello IRCAM di divisione del lavoro tra compositore e tecnologo — una divisione che, come vedremo, solleva questioni profonde sull'authorship musicale.
BYV descrive così il rapporto di lavoro con Boulez:

> "Quando abbiamo iniziato a lavorare insieme, l'ho fatto lavorare sulla macchina. Ma sai, è come uno strumento, devi esercitarti regolarmente. E Pierre era troppo irregolare, quindi i suoi progressi non erano davvero così [buoni]... Fondamentalmente lavoro per lui per un sentimento positivo. Altrimenti, non lavorerei con altri compositori; non sarei un tutor che passa da un universo musicale all'altro, di qualità variabile. Ciò che trovo soddisfacente nel lavoro con Pierre è il senso di continuità nel tempo" [@born1995, pp. 267-268].

Il lavoro di BYV non era semplicemente "assistenza tecnica" ma comportava decisioni musicalmente rilevanti. Come egli stesso ammette, "le soluzioni su cui lavoro con lui porteranno ovviamente in una certa misura parti della mia personalità" [@born1995, p. 268]. Tuttavia, BYV mantiene fermamente che "l'autore indiscusso di Répons o di qualsiasi pezzo su cui lavoro con Pierre è Pierre stesso, non c'è dubbio" [@born1995, p. 268]. Questa affermazione solleva però interrogativi: se BYV è responsabile della concezione e manipolazione della tecnologia, della produzione dei materiali sonori nuovi, il suo contributo all'authorship complessiva del pezzo non è forse "considerevole"? [@born1995, p. 267]
Il modello di lavoro era questo: Boulez dava a BYV "un compito piccolo o grande, e quando tornava ascoltava il risultato" [@born1995, p. 267]. BYV lavorava quindi autonomamente sulla macchina durante le assenze di Boulez — che erano frequenti data la sua carriera di direttore d'orchestra — sviluppando soluzioni tecniche che incorporassero le indicazioni musicali ricevute. Era una collaborazione irregolare ma continuativa, che si estendeva su un arco temporale lungo (quasi cinque anni al momento dell'intervista di Born) e che includeva non solo Répons ma "altri progetti all'orizzonte" e "lavoro su problemi che vanno oltre i pezzi particolari" [@born1995, p. 268].

### Problemi tecnici e dipendenze sistemiche

Ma la produzione di Répons fu tutt'altro che lineare. Nel 1984, in vista della première parigina di ottobre, tutto il lavoro dei diversi team impegnati nel progetto 4X andava verso la preparazione delle prime di Répons, in cui il 4X aveva un "ruolo da protagonista" [@born1995, pp. 186-187]. Tuttavia, IRCAM affrontava in quel momento numerose incertezze tecnologiche. Una fonte di difficoltà erano le negoziazioni difficili per mettere la macchina 4X in produzione industriale, necessarie per fornire abbastanza macchine per le première di Répons [@born1995, p. 105].
L'anno 1984 fu anche un periodo instabile per l'infrastruttura informatica di base. L'anno precedente aveva visto una transizione da un minicomputer DEC PDP10, che aveva servito per diversi anni, alla nuova generazione di macchine: un DEC VAX 780 più il sistema operativo software associato, UNIX [@born1995, pp. 104-105]. Questa combinazione VAX/UNIX era il sistema emergente del momento, sempre più diffuso nella comunità di ricerca internazionale. Ma proprio per questa ragione la tecnologia stava anche evolvendo rapidamente ed era quindi instabile [@born1995, p. 105].
La decisione di aggiornare da UNIX 4.1 a 4.1a fu presa in parte per collegare IRCAM a una nuova importante struttura di rete, ma 4.1a era anche necessario per supportare nuove macchine chiamate "Valids" che, a loro volta, avrebbero agito come workstation per le nuove unità 4X una volta consegnate da Sogitec [@born1995, p. 257]. Poiché questo cambiamento comportava la riscrittura della codifica di base di tutti i programmi in uso sul sistema, era destinato a generare molti bug, quindi ci sarebbero volute settimane per debuggare il sistema e tornare al funzionamento normale [@born1995, p. 257].
Gli ethnographic fieldnotes di Born documentano sessioni di lavoro quotidiane in cui i problemi tecnici dominavano il processo compositivo. In una sessione tipica di aprile 1984, il compositore visitatore AV e il suo tutor XH lavoravano per ore tracciando problemi strani: i misuratori di ampiezza su un paio di canali nel mixer mostravano una fluttuazione — "gli aghi ci salutavano selvaggiamente ma regolarmente quando non veniva emesso alcun suono" [@born1995, p. 244]. Cercavano di tracciarlo: un problema di fase? O a causa del rumore che avevano messo su ogni oscillatore? Tentavano di isolare il problema, ma cambiava ogni volta che provavano qualcosa. Sembrava essere causato dal rumore, perché empiricamente migliorava quando eliminavano l'effetto rumore, ma non riuscivano a capire perché [@born1995, p. 244].
Questi problemi non erano periferici ma centrali al processo. Come osservò ironicamente AV durante una di queste sessioni di debugging: "la computer music è lo stato più alto dell'esperienza umana perché si deve solo impegnarsi e godere del processo di lavoro in sé, piuttosto che del risultato finale di un pezzo: rinunciare alla gratificazione di un risultato" [@born1995, p. 244]. Era una battuta amara che rivelava la frustrazione per un processo in cui la lotta con la tecnologia minacciava costantemente di eclissare la creazione musicale stessa.

### La première come showcase istituzionale

La première parigina di Répons nell'ottobre 1984 in una sala speciale del Centre Georges Pompidou fu concepita come evento istituzionale di massima importanza [@born1995, pp. 104-105, 175-176]. L'opera avrebbe corso per sei notti esaurite in uno spazio appositamente preparato nel CGP, progettata per mostrare non solo al pubblico d'élite della cultura francese, ma anche alla comunità internazionale della computer music la migliore musica e tecnologia di IRCAM [@born1995, p. 105]. Il pubblico della première era indicativo: una fila di posti riservati davanti rimase vuota fino a poco prima dell'inizio, quando un gruppo di figure si precipitò avanti per riempirli, tra cui il leader politico di destra Jacques Chirac — allora sindaco di Parigi, poi Primo Ministro — e Madame Pompidou [@born1995, pp. 175-176].
Ma la première era anche l'apertura della International Computer Music Conference (ICMC), la principale riunione annuale di computer music, che IRCAM ospitava per la prima volta [@born1995, p. 105]. Era quindi un palcoscenico doppio: nazionale e internazionale, politico e scientifico, artistico e tecnologico. Répons doveva funzionare come legittimazione simultanea del progetto IRCAM nel suo complesso, della tecnologia 4X in particolare, e dell'approccio di Boulez alla composizione assistita da tecnologia.
Il ritardo nella consegna delle unità 4X da Sogitec — dovuto in parte al rifiuto del designer BU di fornire progetti scritti per la macchina, costringendo Sogitec a elaborare le sue operazioni da zero — significò che le unità arrivarono "scomodamente vicino" alle première di Répons [@born1995, p. 110]. Questa precarietà tecnica dell'ultimo minuto rivelava una caratteristica strutturale del modello IRCAM: la dipendenza da tecnologie esclusive, complesse e instabili creava una fragilità sistemica che la pressione degli eventi pubblici non faceva che amplificare.
Répons incarnava quindi, nel bene e nel male, il metodo IRCAM in tutta la sua complessità: l'ambizione compositiva monumentale, lo sviluppo tecnologico su misura, la divisione del lavoro gerarchica ma collaborativa, le mediazioni tecnologiche multiple, l'instabilità cronica dei sistemi, la dipendenza da expertise specializzata, e la funzione dell'opera come strumento di legittimazione istituzionale. Era un modello radicalmente diverso da quello che i compositori del Sonic Arts Union avevano elaborato negli stessi anni — un contrasto che il prossimo capitolo esplorerà in dettaglio.

## Limiti del modello: ricerca fondamentale vs produzione musicale

### Tensione research vs production

La contraddizione più profonda del modello IRCAM emerse chiaramente nel 1984, quando un gruppo informale di ricercatori — il cosiddetto "musicians' group" — iniziò a tenere riunioni volontarie per discutere il futuro della music research all'interno dell'istituto [@born1995, pp. 194-221]. Questo gruppo, che includeva i direttori di Music Research e Pedagogy, il direttore del 4X Software, il direttore di Chant/Formes, diversi junior tutors e tutors ufficiali, rappresentava l'élite intellettuale di IRCAM, coloro che si auto-percepivano come l'avanguardia dell'istituto [@born1995, pp. 133-134]. Ma le loro riunioni rivelarono una frattura fondamentale tra due visioni inconciliabili della mission di IRCAM.
Da un lato, il musicians' group sosteneva che "IRCAM deve supportare ricerca fondamentale a lungo termine che sia indipendente dai bisogni immediati di produzione musicale" e che l'istituto "attualmente non riusciva a farlo" [@born1995, p. 219]. La loro posizione associava una catena di termini semanticamente carichi: musica, ricerca, lungo termine, idealismo, software (Chant/Formes), lavoro mentale [@born1995, pp. 219-220]. La ricerca era concepita come "un processo continuo di esperimento e ricerca di conoscenza" [@born1995, p. 219], un percorso aperto che poteva legittimamente "disdegnare risultati immediati" [@born1995, p. 219].
Dall'altro lato, Boulez e il Direttore Scientifico sostenevano che la ricerca dovesse essere "legata alla produzione, a mostrare risultati in un tempo specificato, più breve" [@born1995, p. 219]. Per loro, "produzione" significava "l'attualizzazione della ricerca, lavoro che stabilizza il flusso continuo del processo di ricerca e si allea con un risultato, sia musicale che tecnologico" [@born1995, p. 219]. La posizione di Boulez associava invece: musica, produzione, breve termine, realismo [@born1995, p. 219].
Questa opposizione non era meramente terminologica ma rifletteva una crisi più profonda. Come osservò HM, junior tutor e psicoacustico del musicians' group:

> "L'ala di produzione musicale là, Boulez e [il Direttore Artistico], si rifiutano di capire quale sia questo processo perché non sono stati lì dentro a lottare con lo sviluppo loro stessi. Non vogliono sapere di tutta quella 'spazzatura' [di ricerca], solo del risultato musicale finale. C'è una sorta di impazienza, e capisco. Ma so che per arrivarci, devi passare attraverso certi passaggi" [@born1995, pp. 214-215].


### Composer visits: frustrazioni e mediazioni


Il problema non era astratto: impattava direttamente sulla produzione musicale. Nel 1984, quello che doveva essere un anno di intensa attività compositiva si rivelò invece "eccezionalmente improduttivo" [@born1995, p. 105]. Erano state pianificate quattro commissioni di compositori visitatori, ma solo tre ebbero luogo, e una di queste non risultò in un pezzo [@born1995, p. 105]. L'obiettivo dal 1985 era avere dodici compositori visitatori all'anno [@born1995, p. 105] — un target che rivelava quanto la produzione musicale effettiva fosse ben al di sotto delle aspettative.
Le ragioni erano radicate nel problema della stabilizzazione della ricerca. HM spiegò il dilemma usando Chant come esempio:

> "Nelle prime fasi di Chant era sempre in evoluzione, quindi era molto difficile usarlo come strumento per la produzione. Arrivò un punto in cui decisero che erano andati abbastanza lontano, che qualsiasi cosa ulteriore l'avrebbero spostata in un altro progetto — e quello divenne Formes. Così dopo alcuni anni Chant si fissò in qualche modo, e a quel punto entrò davvero in produzione. Ma ci vollero diversi anni. E nel frattempo la gente diceva: 'Beh, è passato un anno, sai, e non abbiamo strumenti che siano utili!' Non funziona così! Devi arrivare a un certo livello prima di fare una versione che la gente possa usare" [@born1995, pp. 213-214].

Il problema era ancora più acuto con Formes, che "è stato in evoluzione costantemente, ed è stato una vera frustrazione per le persone che lo usavano" [@born1995, p. 214]. La tensione tra ricerca e produzione creava un circolo vizioso: i ricercatori avevano bisogno di tempo per sviluppare sistemi musicalmente potenti, ma i compositori visitatori avevano bisogno di strumenti stabili e documentati per produrre musica nei loro tre mesi di permanenza.
Le note etnografiche di Born documentano questa frustrazione quotidiana. Un compositore visitatore ironizzò amaramente: "la computer music è lo stato più alto dell'esperienza umana perché devi solo impegnarti e godere del processo di lavoro in sé, piuttosto che del risultato finale di un pezzo: rinunciare alla gratificazione di un risultato" [@born1995, p. 244]. Era una battuta che mascherava una critica feroce: il processo tecnologico aveva eclissato l'obiettivo musicale.


### Il dibattito "small systems" interno a IRCAM


Una manifestazione particolare di questa tensione fu il dibattito sui "small systems" — sistemi commerciali accessibili come Apple Macintosh e Yamaha DX7. Il Pedagogy director RIG aveva stabilito contatti con entrambe le compagnie e nell'estate 1984 aveva organizzato un accordo con Apple per ricevere sei Mac gratuitamente in cambio di diritti su software sviluppato con essi [@born1995, pp. 283-284]. La reazione di Boulez fu categorica: riferì a RIG che i Mac sarebbero entrati in IRCAM "sopra il mio cadavere" (over my dead body) [@born1995, p. 284].
L'ostilità non era solo di Boulez. Il musicians' group stesso era diviso sulla questione. Mentre RIG proponeva ricerca su "trasferimento di informazioni da grandi macchine a small systems" come uno dei temi futuri di music research, questa proposta ricevette solo "un supporto debole e retorico" [@born1995, p. 203]. Per molti, i small systems rappresentavano una minaccia alla stessa ragion d'essere di IRCAM.
Il Direttore Scientifico FOL, in una riunione per discutere il documento di proposta del musicians' group, li accusò di essere "troppo astratti, impratici e irreali" e di ignorare il progetto 4X [@born1995, pp. 215-216]. Quando il musicians' group replicò che quelli erano "i temi di ricerca musicale" — implicando che il 4X non fosse music research — il conflitto divenne esplicito [@born1995, p. 216]. FOL ribatté: "Non ho mai visto un 'documento di ricerca' così impratico! È inadeguato. Anche la maggioranza delle persone non presenti qui lo penserebbe. I temi sono troppo grandi, vaghi. Dovete pensare a come si sviluppano nella realtà" [@born1995, p. 216].
3.4.4. L'ambivalenza di Boulez e il fallimento del music research
Ma la critica più devastante venne dalla crescente disillusione di Boulez stesso verso il concetto di music research che lui stesso aveva promosso. In diverse occasioni nelle riunioni del musicians' group, Boulez rimproverò i ricercatori per il loro fallimento nell'orientare il lavoro sufficientemente verso la produzione musicale. In un monologo caratteristico, dichiarò:

> "Non voglio che IRCAM diventi come il CNRS dove i ricercatori si nascondono negli angoli per trent'anni! Voglio che tutors e ricercatori dividano il loro tempo tra ricerca e produzione. E voglio che tutta la ricerca sia legata ai problemi di produzione e realizzazione; così quando arriva un compositore... [Un ricercatore interrompe criticamente: 'Questa è la prospettiva a breve termine!'] [Boulez, liquidandolo] Ho appena iniziato. Per esempio, quando venne Stockhausen, aveva bisogno di ricerca su strumenti per il suo pezzo, vero WR [il tutor]? Voglio che l'anno artistico e l'anno di ricerca siano lo stesso, per sottolineare la natura legata di questi due processi: interazione tra realizzazione e riflessione. Questa è la cosa principale che voglio sottolineare. Abbiamo una responsabilità di restare in contatto con il mondo esterno! Tutti seguono quello che facciamo qui, quindi dobbiamo avere risultati da mostrare per il nostro lavoro" [@born1995, pp. 214-215].

L'ambivalenza di Boulez verso la music research divenne proverbiale. Un ricercatore la descrisse così: "Pierre è difficile da capire, è ambivalente. È d'accordo che ci debba essere la ricerca, ma diventa impaziente se non vede risultati abbastanza presto. Un minuto sta dicendo: 'Quello che viene fatto non serve a niente!' Il minuto dopo ha la sua idea di 'utopie de la recherche musicale' [utopia della ricerca musicale] in corso! [Dubbioso] Dice di esserci impegnato..." [@born1995, p. 216].
Questa ambivalenza fu persino satirizzata in un memo interno con una falsa voce bibliografica: "BU [designer del 4X] e Boulez, P. (1985) — 'La recherche en musique?,' Revue de Neurospeculation, 69: 123-145" [@born1995, p. 216] — una presa in giro del crescente scetticismo sia del designer hardware che di Boulez verso l'intero progetto di music research.


### Un modello insostenibile


Il fallimento del modello IRCAM di integrare ricerca fondamentale e produzione musicale rivelava contraddizioni strutturali più profonde. Come osservò Born, "quelle persone più vicine all'ideale di Boulez — tutors, junior tutors, ricercatori puri orientati alla musica, cioè l'avanguardia di musicisti e scienziati di IRCAM — piuttosto che essere il nucleo meglio supportato dei lavoratori di IRCAM, erano il personale di produzione meno sicuro e moderatamente pagato" [@born1995, pp. 141-142]. L'inversione tra status culturale e sicurezza economica creava una situazione insostenibile: chi faceva il lavoro considerato culturalmente più importante era anche il più precario e mal pagato.
Il modello richiedeva anche una produttività musicale che la complessità tecnologica rendeva impossibile. Mentre IRCAM aveva investito enormi risorse nello sviluppo del 4X, solo Répons e pochi altri pezzi usarono la macchina con successo [@born1995, pp. 140, 186]. La tecnologia esclusiva, invece di democratizzare l'accesso a nuove possibilità compositive, aveva creato un collo di bottiglia: pochi compositori potevano usarla, e quelli che lo facevano dipendevano totalmente da tutors specializzati e da tecnologie cronicamente instabili.
Il contrasto con il modello DIY americano era lampante. Mentre Mumma, Tudor e i loro colleghi lavoravano con tecnologie accessibili, modificabili individualmente e immediatamente utilizzabili in performance, IRCAM aveva creato un sistema che richiedeva mediazione istituzionale, expertise altamente specializzata, cicli di sviluppo lunghi anni, e che produceva risultati musicali sporadici e costosi. Era un modello che, nelle parole di un compositore dissidente interno, faceva del 4X semplicemente "l'Ammiraglia Francese, solo un grande oggetto di prestigio" piuttosto che uno strumento musicale funzionale [@born1995, p. 287].



# CONCLUSIONI

## Sintesi dei percorsi

## Live electronics come specchio di visioni del mondo

## Prospettive future


# BIBLIOGRAFIA

::: {#refs-bib}
:::

# SITOGRAFIA

::: {#refs-sit}
:::

